# 强化学习 (Reinforcement Learning) 指南

这是一份全面的强化学习 (Reinforcement Learning, RL) 指南，从入门到进阶，教你如何使用 Unsloth 和 GRPO 训练你自己的 DeepSeek-R1 推理模型。

## 1. 你将学到什么

这篇文章会为你解答：

-   什么是 RL、RLVR、PPO、GRPO、RLHF、RFT？
-   为什么说"运气是强化学习所需要的一切？"
-   什么是环境 (environment)？智能体 (agent)？动作 (action)？奖励函数 (reward function)？以及奖励 (rewards)？

本文将全面介绍 GRPO、强化学习 (RL) 和奖励函数所需知道的一切（从入门到进阶），并提供一些使用 Unsloth 和 GRPO 的技巧与基础知识。如果你在寻找一个手把手的 GRPO 实战教程，可以参考[这篇指南](https://docs.unsloth.ai/basics/reinforcement-learning-guide/tutorial-train-your-own-reasoning-model-with-grpo)。

## 2. 什么是强化学习 (RL)？

强化学习的目标很简单：

-   增加"好"结果出现的概率。
-   降低"坏"结果出现的概率。

就是这么回事！当然，这里的"好"与"坏"具体指什么，我们如何"增加"或"降低"概率，甚至"结果"本身是什么，都藏着不少细节。

### 2.1. 游戏中的例子：吃豆人

以吃豆人游戏为例：

-   **环境 (Environment)**：就是整个游戏世界。
-   **智能体 (Agent)**：就是玩家，也就是吃豆人。
-   **动作 (Actions)**：你能做的操作，即上、下、左、右。
-   **奖励 (Rewards)**：如果你吃掉一个豆子，这是好的奖励；如果撞上敌人，这就是坏的奖励（或者说惩罚）。

在强化学习中，你无法预知哪一步是"最佳动作"，但你可以观察到过程中的每一步以及最终的游戏结果（赢或输）。

### 2.2. 语言模型的例子：设计奖励函数

再比如，想象你问一个模型："2 + 2 等于几？" 正确答案是 4。一个未对齐的语言模型可能会吐出 3、4、C、D、-10 等等，任何答案都有可能。

-   得到数字比得到字母 C 或 D 要好，对吧？
-   得到 3 比得到 8 要好吧？
-   而得到 4 绝对是正确答案。

看，我们刚刚就设计出了一个**奖励函数 (reward function)**！

## 3. 从 RLHF, PPO 到 GRPO 和 RLVR

OpenAI 普及了 RLHF (Reinforcement Learning from Human Feedback) 的概念，即通过人类反馈来强化学习。我们训练一个"智能体" (agent) 对一个问题（状态, state）生成答案，并由人类来评价哪个答案更有用。

例如，ChatGPT 中的"赞"和"踩"就可以用于 RLHF 过程。

为了实现 RLHF，PPO (Proximal Policy Optimization，近端策略优化) 算法被开发出来。在这种场景下，语言模型就是智能体。实际上，它由三个系统组成：

1.  **生成策略 (Generating Policy)**：当前正在训练的模型。
2.  **参考策略 (Reference Policy)**：原始的、未经训练的模型。
3.  **价值模型 (Value Model)**：用于评估平均奖励的模型。

我们使用奖励模型 (Reward Model) 来计算当前环境下的奖励，并以最大化这个奖励为目标。

PPO 的公式看起来相当复杂，因为它被设计得非常稳定。`clip(..., 1-e, 1+e)` 项用于限制 PPO 的更新步长不要过大。还有一个 KL 项（当 beta > 0 时），用来防止模型偏离原始模型太远。

> 想更深入地了解 PPO 的数学推导，可以访问我们在 AI Engineer 2025 大会上关于 RL 的演讲。

DeepSeek 开发了 GRPO (Group Relative Policy Optimization，组相对策略优化) 来训练他们的 R1 推理模型。与 PPO 的主要区别在于：

1.  **移除了价值模型**：取而代之的是，通过多次调用奖励模型得出的统计数据。
2.  **移除了奖励模型**：取而代之的是，使用一个自定义的奖励函数，RLVR 就可以用在这里。

这意味着 GRPO 非常高效。以前 PPO 需要训练多个模型，现在去掉了奖励模型和价值模型，我们可以节省大量内存并全面提速。

RLVR (Reinforcement Learning with Verifiable Rewards，基于可验证奖励的强化学习) 允许我们根据那些易于验证解决方案的任务来奖励模型。例如：

-   数学方程可以轻松验证。例如 `2+2 = 4`。
-   代码输出可以验证是否正确执行。

设计可验证的奖励函数可能很困难，所以大多数例子都集中在数学或代码上。

GRPO 的应用场景不仅限于代码或数学——它的推理过程可以增强诸如邮件自动化、数据库检索、法律和医学等任务，根据你的数据集和奖励函数极大地提高准确性。这里的诀窍是定义一个"评分标准" (rubric)——即一系列较小的、可验证的奖励，而不是一个最终的、包罗万象的单一奖励。例如，OpenAI 在其强化学习微调 (RFT) 服务中推广了这种做法。

### 3.1. 为什么叫"组相对 (Group Relative)"？

GRPO 完全移除了价值模型，但我们仍然需要估计在当前状态下的"平均奖励"。

诀窍在于对大语言模型 (LLM) 进行采样！我们对多个不同问题进行采样，然后通过统计这些采样过程的奖励来计算平均奖励。

例如，对于问题"2+2 等于几？"，我们采样 4 次，可能得到 `4`, `3`, `D`, `C`。然后我们为每个答案计算奖励，再计算平均奖励和标准差，并对其进行 Z-score 标准化。

这就产生了优势值 A，用来替代价值模型。这节省了大量内存！

## 4. 运气（或者说耐心）是你所需要的一切

强化学习的诀窍只需要两样东西：

1.  一个问题或指令，例如"2+2 等于几？"或"用 Python 写一个 Flappy Bird 游戏"。
2.  一个奖励函数和验证器，用来判断输出是好是坏。

只要有这两样东西，我们基本上可以无限次地调用一个语言模型，直到获得一个好的答案。例如，对于"2+2 等于几？"，一个未经训练的差模型可能会输出：

`0, cat, -10, 1928, 3, A, B, 122, 17, 182, 172, A, C, BAHS, %$, #, 9, -192, 12.31` 然后突然输出了 `4`。

奖励信号之前一直是 0, 0, 0...，突然变成了 1。

所以，通过运气和偶然，RL 在多次尝试 (rollouts) 中找到了正确答案。我们的目标是让好的答案 `4` 更多地出现，而其余的（坏答案）则更少出现。

因此，RL 的目标是要有耐心——从极限的角度看，如果正确答案的概率至少是一个很小的数（而不是零），那么找到它只是一个时间问题——你最终 100% 会遇到正确答案。

所以我喜欢称之为"运气是 RL 所需要的一切"。当然，一个更好的说法是"耐心是 RL 所需要的一切"。

RL 本质上提供了一个捷径——我们不只是无限地等待，我们也能从"坏信号"（即坏答案）中学习，并引导模型尽量不去生成这些坏答案。这意味着，虽然你可能需要等很长时间才能等到一个"好"答案出现，但模型已经在此过程中被调整，尽力避免输出坏答案了。

在"2+2=？"的例子中，RL 会影响模型尽量不去输出那些错误的答案。这意味着随着时间的推移，我们正在小心地"修剪"或移动模型的输出分布，使其远离错误的答案空间。所以 RL 并非效率低下，因为我们不只是在无限等待，而是在积极地"推动"模型尽可能地进入"正确答案空间"。

如果正确答案的概率永远为 0，那么 RL 就永远不会起作用。这也是为什么人们喜欢从一个已经经过指令微调 (instruction finetuned) 的模型开始进行 RL，因为这样的模型已经能够比较好地遵循指令——这很可能将正确答案的概率提升到 0 以上。

## 5. Unsloth 为 RL 提供了什么

仅需 15GB VRAM，Unsloth 就能让你将任何高达 17B 参数的模型（如 Llama 3.1 (8B)、Phi-4 (14B)、Mistral (7B) 或 Qwen2.5 (7B)）转变为一个推理模型。

-   **最低要求**：只需 5GB VRAM，就足以在本地训练你自己的推理模型（适用于任何 1.5B 参数或更小的模型）。

## 6. 教程：用 GRPO 训练你自己的推理模型

GRPO Colab 教程链接:

-   Qwen3 (4B) - 高级
-   DeepSeek-R1-0528-Qwen3-8B - 最新
-   Llama 3.2 (3B) - 高级
-   Gemma 3 (1B)
-   Phi-4 (14B)
-   Qwen2.5 (3B)
-   Mistral v0.3 (7B)
-   Llama 3.1 (8B)

**新功能！** 我们现在支持 Dr. GRPO 和大多数其他新的 GRPO 技术。你可以在 `GRPOConfig` 中调整以下参数来启用它们：

```python
epsilon=0.2,
epsilon_high=0.28, # one sided
delta=1.5 # two sided

loss_type='bnpo',
# or:
loss_type='grpo',
# or:
loss_type='dr_grpo',

mask_truncated_completions=True,
```

如果你的模型没有产生任何推理，请确保你有足够的训练步数，并检查你的奖励函数/验证器是否正常工作。我们在这里提供了[奖励函数的例子](https://docs.unsloth.ai/basics/reinforcement-learning-guide/reward-functions)。

之前的演示表明，你可以在 Qwen2.5 (3B) 上实现自己的"顿悟"时刻——但这需要 2xA100 GPU (160GB VRAM)。现在，使用 Unsloth，你只需一个 5GB VRAM 的 GPU 就能实现同样的"顿悟"。

以前，GRPO 只支持完整微调，但我们已经让它与 QLoRA 和 LoRA 兼容。

例如，在 20K 上下文长度、每个提示生成 8 次的情况下，Unsloth 对 Llama 3.1 (8B) 仅使用 54.3GB VRAM，而标准实现（+ Flash Attention 2）则需要 510.8GB（Unsloth 节省了 90%）。

请注意，这并不是微调 DeepSeek 的 R1 蒸馏模型，也不是使用 R1 的蒸馏数据进行调优（Unsloth 已支持这些）。这是将一个标准模型转换为一个功能齐全的推理模型。

在一个测试示例中，尽管我们只用 GRPO 训练了 Phi-4 100 步，结果已经很明显了。没有经过 GRPO 训练的模型没有 `thinking` token，而经过 GRPO 训练的模型不仅有，而且答案也是正确的。

## 7. 使用 GRPO 进行训练

关于如何使用 Unsloth 和 GRPO 将任何开放的 LLM 转换为推理模型的教程，请看[这里](https://docs.unsloth.ai/basics/reinforcement-learning-guide/tutorial-train-your-own-reasoning-model-with-grpo)。

### 7.1. GRPO 如何训练模型

1.  对于每个问答对，模型会生成多个可能的回答（例如 8 个变体）。
2.  每个回答都由奖励函数进行评估。
3.  模型在每一步通过更新其权重来学习。

**训练步数**：

-   如果你有 300 行数据，那就是 300 个训练步数（如果训练 3 个 epoch，就是 900 步）。
-   你可以增加每个问题生成的回答数量（例如从 8 个增加到 16 个）。

如果你的 GRPO 模型学习效果不佳，我们强烈建议使用我们的高级 GRPO 笔记，因为它有更好的奖励函数，你应该能更快、更频繁地看到效果。

### 7.2. 基础知识/技巧

-   至少等待 300 步，奖励才可能真正开始增加。为了获得不错的结果，你可能需要至少训练 12 小时（GRPO 就是这样工作的），但这并非强制性的，你可以随时停止。
-   为获得最佳效果，至少有 500 行数据。你也可以用 10 行数据进行尝试，但数据越多越好。
-   每次训练的运行都会因你的模型、数据、奖励函数/验证器等而异。所以，虽然我们写了最少 300 步，但有时可能需要 1000 步或更多。
-   如果你在本地使用 GRPO 和 Unsloth，如果遇到错误，请同时 `pip install diffusers`。也请使用最新版本的 vLLM。
-   建议将 GRPO 应用于至少 1.5B 参数的模型，以正确生成 `thinking` token，因为较小的模型可能做不到。
-   对于 QLoRA 4-bit 的 GRPO GPU VRAM 需求，一般规则是：模型参数 ≈ 你需要的 VRAM 量。上下文长度越长，VRAM 需求也越大。LoRA 16-bit 至少会多用 4 倍的 VRAM。
-   可以进行持续微调，你可以让 GRPO 在后台一直运行。
-   在示例 notebook 中，我们使用了 GSM8K 数据集，这是目前 R1 风格训练最流行的选择。
-   如果你使用的是基础模型，请确保你有一个聊天模板 (chat template)。
-   用 GRPO 训练得越多越好。GRPO 最棒的一点是你甚至不需要太多数据。你只需要一个好的奖励函数/验证器，训练时间越长，模型效果就越好。随着时间的推移，你的奖励与步数的关系图应该会像这样上升：

Unsloth 内置了 GRPO 的训练损失跟踪功能，无需使用 wandb 等外部工具。它包含了所有奖励函数的完整日志记录细节，包括聚合后的总奖励函数本身。

## 8. 奖励函数 (Reward Functions) / 验证器 (Verifiers)

在强化学习中，奖励函数和验证器在评估模型输出方面扮演着不同的角色。通常你可以将它们视为同一回事，但严格来说它们并不相同，不过关系不大，因为它们通常是结合使用的。

-   **验证器 (Verifier)**：
    -   判断生成的回答是**正确**还是**错误**。
    -   它不给出数字分数——只做正确性验证。
    -   例如：如果模型对"2+2"生成了"5"，验证器会检查并标记为"错误"。
    -   验证器还可以执行代码（例如 Python）来验证逻辑、语法和正确性，无需手动评估。

-   **奖励函数 (Reward Function)**：
    -   将验证结果（或其他标准）转换为一个**数字分数**。
    -   例如：如果答案错误，可能会给一个惩罚（-1, -2 等），而正确答案则会得到一个正分（+1, +2）。
    -   它还可以根据正确性以外的标准进行惩罚，比如回答过长或可读性差。

**主要区别**：
-   验证器检查正确性，但不打分。
-   奖励函数分配分数，但本身不一定验证正确性。
-   奖励函数可以使用验证器，但它们在技术上是不同的。

### 8.1. 理解奖励函数

GRPO 的主要目标是最大化奖励，并学习答案是如何推导出来的，而不仅仅是记忆和复现训练数据中的回答。

在每个训练步骤中，GRPO 都会调整模型权重以最大化奖励。这个过程会逐步微调模型。

常规微调（不使用 GRPO）只最大化下一个词的预测概率，而没有为奖励进行优化。GRPO 是为奖励函数进行优化，而不仅仅是预测下一个词。

-   你可以在多个 epoch 中重复使用数据。
-   可以预定义默认的奖励函数用于多种用例，或者你可以让 ChatGPT/本地模型为你生成。
-   设计奖励函数或验证器没有唯一正确的方法——可能性是无限的。但是，它们必须设计得当且有意义，因为设计不佳的奖励可能会无意中降低模型性能。

## 9. 奖励函数示例

你可以参考下面的例子。你可以将你的生成结果输入到像 ChatGPT 4o 或 Llama 3.1 (8B) 这样的 LLM 中，并设计一个奖励函数和验证器来评估它。例如，将你的生成结果输入到你选择的 LLM 中，并设定一个规则："如果答案听起来太像机器人，扣 3 分。"这有助于根据质量标准来优化输出。

### 9.1. 示例 1：简单算术任务

-   **问题**："2 + 2"
-   **回答**："4"
-   **奖励函数 1**：
    -   如果检测到数字 → +1
    -   如果没有检测到数字 → -1
-   **奖励函数 2**：
    -   如果数字与正确答案匹配 → +3
    -   如果不匹配 → -3
-   **总奖励**：所有奖励函数的总和。

### 9.2. 示例 2：邮件自动化任务

-   **问题**：一封入站邮件
-   **回答**：一封出站邮件
-   **奖励函数**：
    -   如果回答包含所需关键字 → +1
    -   如果回答与理想回答完全匹配 → +1
    -   如果回答太长 → -1
    -   如果包含了收件人姓名 → +1
    -   如果存在签名块（电话、邮件、地址）→ +1

### 9.3. Unsloth 基于邻近度的奖励函数

如果你看过我们的高级 GRPO Colab 笔记，你会发现我们创建了一个完全从零开始的、基于邻近度的自定义奖励函数。它旨在奖励那些与正确答案更接近的答案。这个灵活的函数可以应用于各种任务。

在我们的例子中，我们在 Qwen3 (Base) 中启用了推理功能，并引导它完成特定任务：

-   应用预微调策略，以避免 GRPO 默认只学习格式的倾向。
-   通过基于正则表达式的匹配来提高评估准确性。
-   创建超越通用提示（如 `think`）的自定义 GRPO 模板，例如 `<start_working_out></end_working_out>`。
-   应用基于邻近度的评分——模型因答案更接近而获得更多奖励（例如，预测 9 比预测 3 更好，而离群值则受到惩罚）。

### 9.4. GSM8K 奖励函数

在我们的其他例子中，我们使用了由 @willccbb 提供的现有 GSM8K 奖励函数，它很受欢迎且被证明相当有效：

-   `correctness_reward_func` – 奖励与标签完全匹配的答案。
-   `int_reward_func` – 鼓励只生成整数的答案。
-   `soft_format_reward_func` – 检查结构，但允许轻微的换行不匹配。
-   `strict_format_reward_func` – 确保响应结构与提示完全匹配，包括换行符。
-   `xmlcount_reward_func` – 确保响应中每个 XML 标签只出现一次。

## 10. 使用 vLLM

你现在可以直接在你的微调流程中使用 vLLM，这大大提高了吞吐量，并允许你同时进行微调和模型推理！在 1x A100 40GB 上，使用 Unsloth 的动态 4-bit 量化 Llama 3.2 3B Instruct 模型，预计可达 4000 tokens/s。在 16GB 的 Tesla T4（免费 Colab GPU）上，你可以达到 300 tokens/s。

我们还神奇地消除了同时加载 vLLM 和 Unsloth 时的双倍内存使用，为 Llama 3.1 8B 节省了约 5GB，为 Llama 3.2 3B 节省了约 3GB。Unsloth 原本可以在 1x 48GB GPU 上微调 Llama 3.3 70B Instruct，其中 Llama 3.3 70B 的权重占用 40GB VRAM。如果我们不消除双倍内存使用，那么同时加载 Unsloth 和 vLLM 将需要 >= 80GB 的 VRAM。

但有了 Unsloth，你仍然可以在 48GB VRAM 内微调并享受快速推理的好处！要使用快速推理，首先安装 vllm，然后用 `fast_inference` 实例化 Unsloth：

```python
pip install unsloth vllm
from unsloth import FastLanguageModel
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Llama-3.2-3B-Instruct",
    fast_inference = True,
)
model.fast_generate(["Hello!"])
```

## 11. GRPO 需求指南

当你使用 Unsloth 进行 GRPO 时，我们通过多种技巧，将 VRAM 使用率与使用 Flash Attention 2 的标准实现相比，智能地降低了 90% 以上！例如，在 20K 上下文长度、每个提示生成 8 次的情况下，Unsloth 对 Llama 3.1 8B 仅使用 54.3GB VRAM，而标准实现则需要 510.8GB。

我们新的高内存效率 GRPO 线性核将内存使用量削减了 8 倍或更多。这节省了 68.5GB 的内存，同时在 `torch.compile` 的帮助下实际上更快了！

我们利用了前段时间发布的智能 Unsloth 梯度检查点算法。它能智能地将中间激活异步卸载到系统 RAM，而速度仅慢 1%。这节省了 52GB 的内存。

与其他包中的实现不同，Unsloth 还与底层的推理引擎（vLLM）共享同一个 GPU / CUDA 内存空间。这节省了 16GB 的内存。

| 指标 | Unsloth | 标准 + FA2 |
| :--- | :--- | :--- |
| 训练内存成本 (GB) | 42GB | 414GB |
| GRPO 内存成本 (GB) | 9.8GB | 78.3GB |
| 推理成本 (GB) | 0GB | 16GB |
| 20K 上下文的 KV 缓存 (GB) | 2.5GB | 2.5GB |
| **总内存使用** | **54.33GB (节省 90%)** | **510.8GB** |


在典型的标准 GRPO 实现中，你需要创建 2 个大小为 (8, 20K) 的 logits 来计算 GRPO 损失。这在 VRAM 中占用了 `2 * 2 字节 * 8 (生成次数) * 20K (上下文长度) * 128256 (词汇表大小) = 78.3GB`。

Unsloth 为长上下文 GRPO 削减了 8 倍的内存使用，所以我们只需要额外的 9.8GB VRAM 用于 20K 上下文长度！

## 12. 进一步阅读

-   Nathan Lambert 的 RLHF 之书是必读！[链接](https://rlhfbook.com/c/11-policy-gradients.html)
-   Yannic Kilcher 的 GRPO Youtube 视频也是必看！[链接](https://www.youtube.com/watch?v=bAWV_yrqx4w)
-   我们在 AI Engineer World's Fair 2025 做了一个 3 小时的研讨会。幻灯片和其他材料在此：[链接](https://docs.unsloth.ai/ai-engineers-2025)
-   Unsloth 的高级 GRPO Colab 教程：[链接](https://docs.unsloth.ai/basics/reinforcement-learning-guide/tutorial-train-your-own-reasoning-model-with-grpo)
-   从基础模型开始的 GRPO Colab 教程：[链接](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb) 