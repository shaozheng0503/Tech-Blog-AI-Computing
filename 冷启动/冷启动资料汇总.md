# Cold start performance

Modal Functions are run in [containers](/docs/guide/images).

If a container is already ready to run your Function, it will be reused.

If not, Modal spins up a new container.
This is known as a _cold start_,
and it is often associated with higher latency.

There are two sources of increased latency during cold starts:

1. inputs may **spend more time waiting** in a queue for a container
   to become ready or "warm".
2. when an input is handled by the container that just started,
   there may be **extra work that only needs to be done on the first invocation**
   ("initialization").

This guide presents techniques and Modal features for reducing the impact of both queueing
and initialization on observed latencies.

If you are invoking Functions with no warm containers
or if you otherwise see inputs spending too much time in the "pending" state,
you should
[target queueing time for optimization](#reduce-time-spent-queueing-for-warm-containers).

If you see some Function invocations taking much longer than others,
and those invocations are the first handled by a new container,
you should
[target initialization for optimization](#reduce-latency-from-initialization).

## Reduce time spent queueing for warm containers

New containers are booted when there are not enough other warm containers to
to handle the current number of inputs.

For example, the first time you send an input to a Function,
there are zero warm containers and there is one input,
so a single container must be booted up.
The total latency for the input will include
the time it takes to boot a container.

If you send another input right after the first one finishes,
there will be one warm container and one pending input,
and no new container will be booted.

Generalizing, there are two factors that affect the time inputs spend queueing:
the time it takes for a container to boot and become warm (which we solve by booting faster)
and the time until a warm container is available to handle an input (which we solve by having more warm containers).

### Warm up containers faster

The time taken for a container to become warm
and ready for inputs can range from seconds to minutes.

Modal's custom container stack has been heavily optimized to reduce this time.
Containers boot in about one second.

But before a container is considered warm and ready to handle inputs,
we need to execute any logic in your code's global scope (such as imports)
or in any
[`modal.enter` methods](/docs/guide/lifecycle-functions).
So if your boots are slow, these are the first places to work on optimization.

For example, you might be downloading a large model from a model server
during the boot process.
You can instead
[download the model ahead of time](/docs/guide/model-weights),
so that it only needs to be downloaded once.

For models in the tens of gigabytes,
this can reduce boot times from minutes to seconds.

### Run more warm containers

It is not always possible to speed up boots sufficiently.
For example, seconds of added latency to load a model may not
be acceptable in an interactive setting.

In this case, the only option is to have more warm containers running.
This increases the chance that an input will be handled by a warm container,
for example one that finishes an input while another container is booting.

Modal currently exposes [three parameters](/docs/guide/scale) that control how
many containers will be warm: `scaledown_window`, `min_containers`,
and `buffer_containers`.

All of these strategies can increase the resources consumed by your Function
and so introduce a trade-off between cold start latencies and cost.

#### Keep containers warm for longer with `scaledown_window`

Modal containers will remain idle for a short period before shutting down. By
default, the maximum idle time is 60 seconds. You can configure this by setting
the `scaledown_window` on the [`@function`](/docs/reference/modal.App#function)
decorator. The value is measured in seconds, and it can be set anywhere between
two seconds and twenty minutes.

```python
import modal

app = modal.App()

@app.function(scaledown_window=300)
def my_idle_greeting():
    return {"hello": "world"}
```

Increasing the `scaledown_window` reduces the chance that subsequent requests
will require a cold start, although you will be billed for any resources used
while the container is idle (e.g., GPU reservation or residual memory
occupancy). Note that containers will not necessarily remain alive for the
entire window, as the autoscaler will scale down more agressively when the
Function is substantially over-provisioned.

#### Overprovision resources with `min_containers` and `buffer_containers`

Keeping already warm containers around longer doesn't help if there are no warm
containers to begin with, as when Functions scale from zero.

To keep some containers warm and running at all times, set the `min_containers`
value on the [`@function`](/docs/reference/modal.App#function) decorator. This
puts a floor on the the number of containers so that the Function doesn't scale
to zero. Modal will still scale up and spin down more containers as the
demand for your Function fluctuates above the `min_containers` value, as usual.

While `min_containers` overprovisions containers while the Function is idle,
`buffer_containers` provisions extra containers while the Function is active.
This "buffer" of extra containers will be idle and ready to handle inputs if
the rate of requests increases. This parameter is particularly useful for
bursty request patterns, where the arrival of one input predicts the arrival of more inputs,
like when a new user or client starts hitting the Function.

```python
import modal

app = modal.App(image=modal.Image.debian_slim().pip_install("fastapi"))

@app.function(min_containers=3, buffer_containers=3)
def my_warm_greeting():
    return "Hello, world!"
```

## Reduce latency from initialization

Some work is done the first time that a function is invoked
but can be used on every subsequent invocation.
This is
[_amortized work_](https://www.cs.cornell.edu/courses/cs312/2006sp/lectures/lec18.html)
done at initialization.

For example, you may be using a large pre-trained model
whose weights need to be loaded from disk to memory the first time it is used.

This results in longer latencies for the first invocation of a warm container,
which shows up in the application as occasional slow calls: high tail latency or elevated p9Xs.

### Move initialization work out of the first invocation

Some work done on the first invocation can be moved up and completed ahead of time.

Any work that can be saved to disk, like
[downloading model weights](/docs/guide/model-weights),
should be done as early as possible. The results can be included in the
[container's Image](/docs/guide/images)
or saved to a
[Modal Volume](/docs/guide/volumes).

Some work is tricky to serialize, like spinning up a network connection or an inference server.
If you can move this initialization logic out of the function body and into the global scope or a
[container `enter` method](https://modal.com/docs/guide/lifecycle-functions#enter),
you can move this work into the warm up period.
Containers will not be considered warm until all `enter` methods have completed,
so no inputs will be routed to containers that have yet to complete this initialization.

For more on how to use `enter` with machine learning model weights, see
[this guide](/docs/guide/model-weights).

Note that `enter` doesn't get rid of the latency --
it just moves the latency to the warm up period,
where it can be handled by
[running more warm containers](#run-more-warm-containers).

### Share initialization work across cold starts with memory snapshots

Cold starts can also be made faster by using memory snapshots.

Invocations of a Function after the first
are faster in part because the memory is already populated
with values that otherwise need to be computed or read from disk,
like the contents of imported libraries.

Memory snapshotting captures the state of a container's memory
at user-controlled points after it has been warmed up
and reuses that state in future boots, which can substantially
reduce cold start latency penalties and warm up period duration.

Refer to the [memory snapshot](/docs/guide/memory-snapshot)
guide for details.

### Optimize initialization code

Sometimes, there is nothing to be done but to speed this work up.

Here, we share specific patterns that show up in optimizing initialization
in Modal Functions.

#### Load multiple large files concurrently

Often Modal applications need to read large files into memory (eg. model
weights) before they can process inputs. Where feasible these large file
reads should happen concurrently and not sequentially. Concurrent IO takes
full advantage of our platform's high disk and network bandwidth
to reduce latency.

One common example of slow sequential IO is loading multiple independent
Huggingface `transformers` models in series.

```python notest
from transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForConditionalGeneration
model_a = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor_a = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
model_b = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
processor_b = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large")
```

The above snippet does four `.from_pretrained` loads sequentially.
None of the components depend on another being already loaded in memory, so they
can be loaded concurrently instead.

They could instead be loaded concurrently using a function like this:

```python notest
from concurrent.futures import ThreadPoolExecutor, as_completed
from transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForConditionalGeneration

def load_models_concurrently(load_functions_map: dict) -> dict:
    model_id_to_model = {}
    with ThreadPoolExecutor(max_workers=len(load_functions_map)) as executor:
        future_to_model_id = {
            executor.submit(load_fn): model_id
            for model_id, load_fn in load_functions_map.items()
        }
        for future in as_completed(future_to_model_id.keys()):
            model_id_to_model[future_to_model_id[future]] = future.result()
    return model_id_to_model

components = load_models_concurrently({
    "clip_model": lambda: CLIPModel.from_pretrained("openai/clip-vit-base-patch32"),
    "clip_processor": lambda: CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32"),
    "blip_model": lambda: BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large"),
    "blip_processor": lambda: BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large")
})
```

If performing concurrent IO on large file reads does _not_ speed up your cold
starts, it's possible that some part of your function's code is holding the
Python [GIL](https://wiki.python.org/moin/GlobalInterpreterLock) and reducing
the efficacy of the multi-threaded executor.







ç¼“å­˜æƒé‡ ğŸ†•
é€šè¿‡é¢„å–å’Œç¼“å­˜æƒé‡æ¥åŠ é€Ÿå†·å¯åŠ¨å’Œå¯ç”¨æ€§ã€‚

â€‹
ä»€ä¹ˆæ˜¯â€œå†·å¯åŠ¨â€ï¼Ÿ
â€œå†·å¯åŠ¨â€æ˜¯ä¸€ä¸ªæœ¯è¯­ï¼Œç”¨äºæè¿°ä»æ¨¡å‹ç¼©æ”¾åˆ° 0 ç›´åˆ°å‡†å¤‡å¥½å¤„ç†ç¬¬ä¸€ä¸ªè¯·æ±‚æ‰€èŠ±è´¹çš„æ—¶é—´ã€‚æ­¤è¿‡ç¨‹æ˜¯ä½¿æ‚¨çš„éƒ¨ç½²èƒ½å¤Ÿå“åº”æµé‡ï¼ŒåŒæ—¶ä¿æŒ SLA å¹¶é™ä½æˆæœ¬çš„å…³é”®å› ç´ ã€‚ ä¸ºäº†ä¼˜åŒ–å†·å¯åŠ¨ï¼Œæˆ‘ä»¬å°†ä»‹ç»ä»¥ä¸‹ç­–ç•¥ï¼šåœ¨æ¨¡å—å¯¼å…¥æœŸé—´è¿è¡Œçš„ Rust åå°çº¿ç¨‹ä¸­ä¸‹è½½å®ƒä»¬ï¼Œåœ¨åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿä¸­ç¼“å­˜æƒé‡ï¼Œå¹¶å°†æƒé‡ç§»åŠ¨åˆ° docker é•œåƒä¸­ã€‚

åœ¨å®è·µä¸­ï¼Œè¿™ä¼šå°†å¤§å‹æ¨¡å‹çš„å†·å¯åŠ¨æ—¶é—´ç¼©çŸ­åˆ°å‡ ç§’é’Ÿã€‚ä¾‹å¦‚ï¼ŒStable Diffusion XL å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ‰èƒ½å¯åŠ¨ï¼Œè€Œæ— éœ€ç¼“å­˜ã€‚ä½¿ç”¨ç¼“å­˜æ—¶ï¼Œåªéœ€ä¸åˆ° 10 ç§’ã€‚

â€‹
ä¸ºæ¨¡å‹å¯ç”¨ç¼“å­˜ + é¢„å–
è¦å¯ç”¨ç¼“å­˜ï¼Œåªéœ€å°† .æœ‰å‡ ä¸ªå…³é”®é…ç½®ï¼šmodel_cacheconfig.yamlrepo_idmodel_cache

repo_idï¼ˆå¿…éœ€ï¼‰ï¼šæ¥è‡ª Hugging Face çš„å­˜å‚¨åº“åç§°ã€‚
revisionï¼ˆå¿…éœ€ï¼‰ï¼šhuggingface å­˜å‚¨åº“çš„ä¿®è®¢ç‰ˆï¼Œä¾‹å¦‚ sha æˆ–åˆ†æ”¯åç§°ï¼Œä¾‹å¦‚ æˆ– ã€‚refs/pr/1main
use_volumeï¼šå¸ƒå°”æ ‡å¿—ï¼Œç”¨äºç¡®å®šæƒé‡æ˜¯åœ¨è¿è¡Œæ—¶ä¸‹è½½åˆ° Baseten åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿï¼ˆæ¨èï¼‰è¿˜æ˜¯æ†ç»‘åˆ°å®¹å™¨æ˜ åƒä¸­ï¼ˆæ—§ç‰ˆï¼Œä¸æ¨èï¼‰ã€‚
volume_folderï¼šå­—ç¬¦ä¸²ï¼Œæ¨¡å‹æƒé‡æ˜¾ç¤ºåœ¨å…¶ä¸‹çš„æ–‡ä»¶å¤¹åç§°ã€‚å°†å…¶è®¾ç½®ä¸º å°†åœ¨è¿è¡Œæ—¶å°†å­˜å‚¨åº“æŒ‚è½½åˆ°ã€‚my-llama-model/app/model_cache/my-llama-model
allow_patternsï¼šä»…ç¼“å­˜ä¸æŒ‡å®šæ¨¡å¼åŒ¹é…çš„æ–‡ä»¶ã€‚ä½¿ç”¨ Unix shell æ ·å¼é€šé…ç¬¦æ¥è¡¨ç¤ºè¿™äº›æ¨¡å¼ã€‚
ignore_patternsï¼šç›¸åï¼Œæ‚¨ä¹Ÿå¯ä»¥è¡¨ç¤ºè¦å¿½ç•¥çš„æ–‡ä»¶æ¨¡å¼ï¼Œä»è€Œç®€åŒ–ç¼“å­˜è¿‡ç¨‹ã€‚
è¿™æ˜¯ä¸º Stable Diffusion XL ç¼–å†™çš„ Well ç¤ºä¾‹ã€‚è¯·æ³¨æ„ï¼Œå®ƒå¦‚ä½•ä½¿ç”¨ .model_cacheallow_patterns

config.yaml

å¤åˆ¶

è¯¢é—® AI
model_cache:
  - repo_id: madebyollin/sdxl-vae-fp16-fix
    revision: 207b116dae70ace3637169f1ddd2434b91b3a8cd
    use_volume: true
    volume_folder: sdxl-vae-fp16
    allow_patterns:
      - config.json
      - diffusion_pytorch_model.safetensors
  - repo_id: stabilityai/stable-diffusion-xl-base-1.0
    revision: 462165984030d82259a11f4367a4eed129e94a7b
    use_volume: true
    volume_folder: stable-diffusion-xl-base
    allow_patterns:
      - "*.json"
      - "*.fp16.safetensors"
      - sd_xl_base_1.0.safetensors
  - repo_id: stabilityai/stable-diffusion-xl-refiner-1.0
    revision: 5d4cfe854c9a9a87939ff3653551c2b3c99a4356
    use_volume: true
    volume_folder: stable-diffusion-xl-refiner
    allow_patterns:
      - "*.json"
      - "*.fp16.safetensors"
      - sd_xl_refiner_1.0.safetensors
è®¸å¤š Hugging Face å­˜å‚¨åº“å…·æœ‰ä¸åŒæ ¼å¼ï¼ˆã€ã€ã€ ç­‰ï¼‰çš„æ¨¡å‹æƒé‡ã€‚å¤§å¤šæ•°æ—¶å€™ï¼Œæ‚¨åªéœ€è¦å…¶ä¸­ä¹‹ä¸€ã€‚ä¸ºäº†æœ€å¤§é™åº¦åœ°å‡å°‘å†·å¯åŠ¨ï¼Œè¯·ç¡®ä¿ä»…ç¼“å­˜æ‰€éœ€çš„æƒé‡ã€‚.bin.safetensors.h5.msgpack

â€‹
ä»€ä¹ˆæ˜¯æƒé‡â€œé¢„å–â€ï¼Ÿ
ä½¿ç”¨ ï¼Œæƒé‡æ˜¯é€šè¿‡åœ¨ä¸“ç”¨çš„ Rust çº¿ç¨‹ä¸­æå‰ä¸‹è½½æƒé‡æ¥é¢„å–çš„ã€‚ è¿™æ„å‘³ç€ï¼Œæ‚¨å¯ä»¥æ‰§è¡Œå„ç§å‡†å¤‡å·¥ä½œï¼ˆå¯¼å…¥åº“ã€torch/triton æ¨¡å—çš„ jit ç¼–è¯‘ï¼‰ï¼Œç›´åˆ°æ‚¨éœ€è¦è®¿é—®è¿™äº›æ–‡ä»¶ä¸ºæ­¢ã€‚ åœ¨å®è·µä¸­ï¼Œæ‰§è¡Œè¿™æ ·çš„è¯­å¥é€šå¸¸éœ€è¦ 10-15 ç§’ã€‚å±Šæ—¶ï¼Œå‰ 5-10GB çš„æƒé‡å·²ç»ä¸‹è½½å®Œæ¯•ã€‚model_cacheimport tensorrt_llm

è¦å°†é…ç½®ä¸ truss ä¸€èµ·ä½¿ç”¨ï¼Œæˆ‘ä»¬è¦æ±‚æ‚¨ä¸»åŠ¨ä¸ . åœ¨ä½¿ç”¨ä»»ä½•ä¸‹è½½çš„æ–‡ä»¶ä¹‹å‰ï¼Œæ‚¨å¿…é¡»è°ƒç”¨ .è¿™å°†é˜»æ­¢ï¼Œç›´åˆ°ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶éƒ½ä¸‹è½½å¹¶å‡†å¤‡å¥½ä½¿ç”¨ã€‚ æ­¤è°ƒç”¨å¿…é¡»æ˜¯ your æˆ– implementation çš„ä¸€éƒ¨åˆ†ã€‚model_cachelazy_data_resolverlazy_data_resolver.block_until_download_complete()/app/model_cache__init__load

model.py

å¤åˆ¶

è¯¢é—® AI
# <- download is invoked before here.
import torch # this line usually takes 2-5 seconds.
import tensorrt_llm # this line usually takes 10-15 seconds
import onnxruntime # this line usually takes 5-10 seconds

class Model:
    """example usage of `model_cache` in truss"""
    def __init__(self, *args, **kwargs):
        # `lazy_data_resolver` is passed as keyword-argument in init
        self._lazy_data_resolver = kwargs["lazy_data_resolver"]

    def load():
        # work that does not require the download may be done beforehand
        random_vector = torch.randn(1000)
        # important to collect the download before using any incomplete data
        self._lazy_data_resolver.block_until_download_complete()
        # after the call, you may use the /app/model_cache directory and the contents
        torch.load(
            "/app/model_cache/stable-diffusion-xl-base/model.fp16.safetensors"
        )
â€‹
ç§æœ‰ Hugging Face å­˜å‚¨åº“ ğŸ¤—
å¯¹äºä»»ä½•å…¬å…± Hugging Face å­˜å‚¨åº“ï¼Œæ‚¨æ— éœ€æ‰§è¡Œä»»ä½•å…¶ä»–ä½œã€‚æ·»åŠ å¸¦æœ‰ appropriate çš„å¯†é’¥åº”è¯¥å°±è¶³å¤Ÿäº†ã€‚model_cacherepo_id

ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æƒ³å°†æ¨¡å‹ä» Llama 2 ç­‰é—¨æ§ä»“åº“éƒ¨ç½²åˆ° Basetenï¼Œåˆ™éœ€è¦é‡‡å–ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š

1
è·å– Hugging Face API å¯†é’¥

ä» Hugging Face è·å–å…·æœ‰è®¿é—®æƒé™çš„ API å¯†é’¥ã€‚ç¡®ä¿æ‚¨æœ‰æƒè®¿é—®è¦æœåŠ¡çš„æ¨¡å‹ã€‚read

2
å°†å…¶æ·»åŠ åˆ° Baseten Secrets Manager

å°†æ‚¨çš„ API å¯†é’¥ç²˜è´´åˆ° Baseten ä¸­å¯†é’¥ç®¡ç†å™¨çš„å¯†é’¥ .æ‚¨å¯ä»¥åœ¨æ­¤å¤„é˜…è¯»æœ‰å…³ secret çš„æ›´å¤šä¿¡æ¯ã€‚hf_access_token

3
æ›´æ–°é…ç½®

åœ¨ Truss's ä¸­ï¼Œæ·»åŠ ä»¥ä¸‹ä»£ç ï¼šconfig.yaml

config.yaml

å¤åˆ¶

è¯¢é—® AI
secrets:
  hf_access_token: null
ç¡®ä¿å¯†é’¥åœ¨æ‚¨çš„ .secretsconfig.yaml

å¦‚æœæ‚¨é‡åˆ°ä»»ä½•é—®é¢˜ï¼Œè¯·å†æ¬¡æ‰§è¡Œä¸Šè¿°æ‰€æœ‰æ­¥éª¤ï¼Œå¹¶ç¡®ä¿æ‚¨æ²¡æœ‰æ‹¼é”™å­˜å‚¨åº“çš„åç§°æˆ–ç²˜è´´ä¸æ­£ç¡®çš„ API å¯†é’¥ã€‚

â€‹
model_cacheåœ¨ Chains ä¸­
è¦ç”¨äºé“¾ - è¯·ä½¿ç”¨è¯´æ˜ç¬¦ã€‚åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä¸‹è½½ . ç”±äºæ­¤æ¨¡å‹æ˜¯å°é—­å¼ huggingface æ¨¡å‹ï¼Œå› æ­¤æˆ‘ä»¬å°†æŒ‚è½½ä»¤ç‰Œè®¾ç½®ä¸ºèµ„äº§çš„ä¸€éƒ¨åˆ†ã€‚ è¯¥æ¨¡å‹éå¸¸å° - åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†èƒ½å¤Ÿåœ¨è¿è¡Œæ—¶ä¸‹è½½æ¨¡å‹ã€‚model_cacheAssetsllama-3.2-1Bchains.Assets(..., secret_keys=["hf_access_token"])from transformers import pipelineimport torch

chain_cache.py

å¤åˆ¶

è¯¢é—® AI
import random
import truss_chains as chains

try:
    # imports on global level for PoemGeneratorLM, to save time during the download.
    from transformers import pipeline
    import torch
except ImportError:
    # RandInt does not have these dependencies.
    pass

class RandInt(chains.ChainletBase):
    async def run_remote(self, max_value: int) -> int:
        return random.randint(1, max_value)

@chains.mark_entrypoint
class PoemGeneratorLM(chains.ChainletBase):
    from truss import truss_config
    LLAMA_CACHE = truss_config.ModelRepo(
        repo_id="meta-llama/Llama-3.2-1B-Instruct",
        revision="c4219cc9e642e492fd0219283fa3c674804bb8ed",
        use_volume=True,
        volume_folder="llama_mini",
        ignore_patterns=["*.pth", "*.onnx"]
    )
    remote_config = chains.RemoteConfig(
        docker_image=chains.DockerImage(
            # The phi model needs some extra python packages.
            pip_requirements=[
                "transformers==4.48.0",
                "torch==2.6.0",
            ]
        ),
        compute=chains.Compute(
            gpu="L4"
        ),
        # The phi model needs a GPU and more CPUs.
        # compute=chains.Compute(cpu_count=2, gpu="T4"),
        # Cache the model weights in the image
        assets=chains.Assets(cached=[LLAMA_CACHE], secret_keys=["hf_access_token"]),
    )
    # <- Download happens before __init__ is called.
    def __init__(self, rand_int=chains.depends(RandInt, retries=3)) -> None:
        self._rand_int = rand_int
        print("loading cached llama_mini model")
        self.pipeline = pipeline(
            "text-generation",
            model=f"/app/model_cache/llama_mini",
        )

    async def run_remote(self, max_value: int = 3) -> str:
        num_repetitions = await self._rand_int.run_remote(max_value)
        print("writing poem with num_repetitions", num_repetitions)
        poem = str(self.pipeline(
            text_inputs="Write a beautiful and descriptive poem about the ocean. Focus on its vastness, movement, and colors.",
            max_new_tokens=150,
            do_sample=True,
            return_full_text=False,
            temperature=0.7,
            top_p=0.9,
        )[0]['generated_text'])
        return poem * num_repetitions
â€‹
model_cacheç”¨äºè‡ªå®šä¹‰æœåŠ¡å™¨
å¦‚æœæ‚¨ä¸ä½¿ç”¨ Python å’Œè‡ªå®šä¹‰æœåŠ¡å™¨ï¼Œä¾‹å¦‚ vllmã€TEI æˆ– sglangï¼Œ æ‚¨éœ€è¦ä½¿ç”¨å‘½ä»¤ æ¥å¼ºåˆ¶å¡«å……è¯¥ä½ç½®ã€‚è¯¥å‘½ä»¤å°†é˜»æ­¢ï¼Œç›´åˆ°ä¸‹è½½æƒé‡ã€‚model.pytruss-transfer-cli/app/model_cache

ä¸‹é¢æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œè¯´æ˜å¦‚ä½•åœ¨ L4 ä¸Šä½¿ç”¨ text-embeddings-inference å°† jina åµŒå…¥æ¨¡å‹ä» huggingface å¡«å……åˆ° model_cache ä¸­ã€‚

config.yaml

å¤åˆ¶

è¯¢é—® AI
base_image:
  image: baseten/text-embeddings-inference-mirror:89-1.6
docker_server:
  liveness_endpoint: /health
  predict_endpoint: /v1/embeddings
  readiness_endpoint: /health
  server_port: 7997
  # using `truss-transfer-cli` to download the weights to `cached_model`
  start_command: bash -c "truss-transfer-cli && text-embeddings-router --port 7997
    --model-id /app/model_cache/my_jina --max-client-batch-size 128 --max-concurrent-requests
    128 --max-batch-tokens 16384 --auto-truncate"
model_cache:
- repo_id: jinaai/jina-embeddings-v2-base-code
  revision: 516f4baf13dec4ddddda8631e019b5737c8bc250
  use_volume: true
  volume_folder: my_jina
  ignore_patterns: ["*.onnx"]
model_metadata:
  example_model_input:
    encoding_format: float
    input: text string
    model: model
model_name: TEI-jinaai-jina-embeddings-v2-base-code-truss-example
resources:
  accelerator: L4
â€‹
åœ¨å¯ç”¨ b10cache çš„æƒ…å†µä¸‹è¿›ä¸€æ­¥ä¼˜åŒ–è®¿é—®æ—¶é—´
B10Cache ç›®å‰å¤„äº Beta ç‰ˆ
ä¸ºäº†è¿›ä¸€æ­¥å‡å°‘æƒé‡åŠ è½½æ—¶é—´ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºæ‚¨çš„è´¦æˆ·å¯ç”¨ Baseten çš„åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ ï¼ˆb10cacheï¼‰ã€‚ æ‚¨å¯ä»¥é€šè¿‡æŸ¥çœ‹éƒ¨ç½²æ—¥å¿—æ¥éªŒè¯æ˜¯å¦å·²ä¸ºæ‚¨çš„è´¦æˆ·å¯ç”¨æ­¤åŠŸèƒ½ã€‚


å¤åˆ¶

è¯¢é—® AI
[2025-09-10 01:04:35] [INFO ] b10cache is enabled.
[2025-09-10 01:04:35] [INFO ] Symlink created successfully. Skipping download for /app/model_cache/cached_model/model.safetensors
ä¸€æ—¦ b10cache å¤„äºæ´»åŠ¨çŠ¶æ€ï¼Œæˆ‘ä»¬å°†è·³è¿‡ç¼“å­˜åœ¨è¿è¡Œéƒ¨ç½²çš„åŒºåŸŸçš„åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿä¸­çš„ä¸‹è½½ã€‚ b10cache å°±åƒä¸€ä¸ªå†…å®¹åˆ†å‘ç½‘ç»œï¼šåˆå§‹ç¼“å­˜æœªå‘½ä¸­ä¼šå¡«å……æ–‡ä»¶ç³»ç»Ÿï¼Œæœªä½¿ç”¨çš„æ–‡ä»¶ä¼šåœ¨ä¸Šæ¬¡ä½¿ç”¨ 4 å¤©åè¿›è¡Œåƒåœ¾å›æ”¶ã€‚ ä¸€æ—¦ b10cache å¤„äºæ´»åŠ¨çŠ¶æ€ï¼Œå®ƒå°†ä»æœ€å¿«çš„æºæ‹‰å–ã€‚å¦‚æœå¦ä¸€ä¸ª Pod åœ¨åŒä¸€ç‰©ç†èŠ‚ç‚¹ä¸Šå¤„äºæ´»åŠ¨çŠ¶æ€ï¼Œåˆ™æ„ä»¶å¯èƒ½ä¼šè¢«çƒ­ç¼“å­˜ï¼Œå¹¶åœ¨æ‚¨çš„éƒ¨ç½²ä¹‹é—´å…±äº«ã€‚ ä¸‹è½½ä¸å…¶ä»–ç»„ç»‡å®Œå…¨éš”ç¦»ã€‚ä¸å»ºè®®å°±åœ°/ä¸å¤åˆ¶ä¿®æ”¹ä¸‹è½½çš„æ„ä»¶ã€‚

å¦‚æœ b10cache ä¸é€‚ç”¨äºæ‚¨çš„è´¦æˆ·ï¼Œæˆ‘ä»¬å°†ä¸ºmodel_cacheæä¾›æ¥è‡ª HuggingFace.co çš„ä¼˜åŒ–ä¸‹è½½ã€‚ ä¸‹è½½æ˜¯å¹¶è¡Œçš„ï¼Œåœ¨ 10Gbit ä»¥å¤ªç½‘è¿æ¥ä¸Šå®ç°è¶…è¿‡ 1GB/s çš„å…¸å‹ä¸‹è½½é€Ÿåº¦ã€‚ å¦‚æœæ‚¨æƒ³å¯ç”¨ b10cacheï¼Œè¯·éšæ—¶è”ç³»æˆ‘ä»¬çš„æ”¯æŒäººå‘˜ã€‚

â€‹
æ—§ç‰ˆç¼“å­˜ - å®¹å™¨ä¸­çš„æƒé‡
ç¡®ä¿æƒé‡å§‹ç»ˆå¯ç”¨çš„ä¸€ç§è¾ƒæ…¢çš„æ–¹æ³•æ˜¯åœ¨æ„å»ºæ—¶å°†å®ƒä»¬ä¸‹è½½åˆ° docker é•œåƒä¸­ã€‚ æˆ‘ä»¬ä»…å»ºè®®å¯¹æœ€å¤§ ~1GB çš„å°å‹æ¨¡å‹æ‰§è¡Œæ­¤ä½œã€‚

æƒè¡¡ï¼š

æœ€é«˜å¯ç”¨æ€§ï¼šæ¨¡å‹æƒé‡æ°¸è¿œä¸ä¼šä¾èµ–äº S3/huggingface æ­£å¸¸è¿è¡Œæ—¶é—´ã€‚b10cache ä¸Šçš„é«˜å¯ç”¨æ€§ã€‚
è¾ƒæ…¢çš„å†·å¯åŠ¨ï¼šå¯èƒ½éœ€è¦ä»é€Ÿåº¦è¾ƒæ…¢çš„ S3 æˆ– Huggingface é€Ÿåº¦è¾ƒæ…¢çš„æºä¸­æå– Docker æ˜ åƒã€‚
ä¸é€‚åˆéå¸¸å¤§çš„æ¨¡å‹ï¼šæˆ‘ä»¬ä¸å»ºè®®å°†å¤§å‹æ¨¡å‹æ„ä»¶æ”¾å…¥ docker é•œåƒä¸­ï¼Œå½“å¤§äº 50GB æ—¶ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ„å»ºå¤±è´¥ã€‚
â€‹
é€šè¿‡ä»¥ä¸‹æ–¹å¼å°†æƒé‡ä¸‹è½½åˆ°å›¾åƒä¸­build_commands
å°†æƒé‡ä¸‹è½½åˆ° docker æ˜ åƒçš„æœ€çµæ´»æ–¹æ³•æ˜¯ä½¿ç”¨ custom ã€‚ æ‚¨å¯ä»¥åœ¨æ­¤å¤„é˜…è¯»æœ‰å…³ build_commands çš„æ›´å¤šä¿¡æ¯ã€‚build_commands

config.yaml

å¤åˆ¶

è¯¢é—® AI
build_commands:
- 'apt-get install git git-lfs'
- 'git lfs install'
- 'git clone https://huggingface.co/nomic-ai/nomic-embed-text-v1.5 /data/local-model'
- echo 'Model downloaded to /data/local-model via git clone'
â€‹
é€šè¿‡ å’Œ ä¸‹è½½æƒé‡model_cacheuse_volume: false
å¦‚æœæ‚¨è®¾ç½® ï¼Œæˆ‘ä»¬å°†ä¸ä¼šåœ¨è¿è¡Œæ—¶ä½¿ç”¨ b10cache æ¥æŒ‚è½½æ¨¡å‹æƒé‡ï¼Œè€Œæ˜¯å°†å®ƒä»¬ä¾›åº”å•†åˆ° docker é•œåƒä¸­ã€‚use_volume: false

â€‹
æ‹¥æŠ±è„¸
config.yaml

å¤åˆ¶

è¯¢é—® AI
model_cache:
  - repo_id: madebyollin/sdxl-vae-fp16-fix
    revision: 207b116dae70ace3637169f1ddd2434b91b3a8cd
    use_volume: false
    allow_patterns:
      - config.json
      - diffusion_pytorch_model.safetensors
æƒé‡å°†ç¼“å­˜åœ¨é»˜è®¤çš„ Hugging Face ç¼“å­˜ç›®å½• .æ‚¨å¯ä»¥é€šè¿‡åœ¨ .~/.cache/huggingface/hub/models--{your_model_name}/HF_HOMEHUGGINGFACE_HUB_CACHEconfig.yaml

åœ¨æ­¤å¤„é˜…è¯»æ›´å¤šå†…å®¹ã€‚

Huggingface åº“å°†ç›´æ¥ä½¿ç”¨å®ƒã€‚

model.py

å¤åˆ¶

è¯¢é—® AI
from transformers import AutoModel

AutoModel.from_pretrained("madebyollin/sdxl-vae-fp16-fix")
â€‹
Google äº‘å­˜å‚¨
å½“æ‚¨æ‹¥æœ‰è‡ªå®šä¹‰æ¨¡å‹æˆ–è¦è¿›è¡Œå¾®è°ƒæ—¶ï¼ŒGoogle Cloud Storage æ˜¯ Hugging Face çš„ç»ä½³æ›¿ä»£å“ï¼Œç‰¹åˆ«æ˜¯å¦‚æœæ‚¨å·²ç»åœ¨ä½¿ç”¨ GCP å¹¶å…³å¿ƒå®‰å…¨æ€§å’Œåˆè§„æ€§ã€‚

æ‚¨çš„åº”å¦‚ä¸‹æ‰€ç¤ºï¼šmodel_cache

config.yaml

å¤åˆ¶

è¯¢é—® AI
model_cache:
  - repo_id: gs://path-to-my-bucket
    use_volume: false
å¦‚æœæ‚¨æ­£åœ¨è®¿é—®å…¬æœ‰ GCS å­˜å‚¨æ¡¶ï¼Œåˆ™å¯ä»¥å¿½ç•¥ä»¥ä¸‹æ­¥éª¤ï¼Œä½†è¯·ç¡®ä¿åœ¨å­˜å‚¨æ¡¶ä¸Šè®¾ç½®é€‚å½“çš„æƒé™ã€‚ç”¨æˆ·åº”è¯¥èƒ½å¤Ÿåˆ—å‡ºå’ŒæŸ¥çœ‹æ‰€æœ‰æ–‡ä»¶ã€‚å¦åˆ™ï¼Œæ¨¡å‹æ„å»ºå°†å¤±è´¥ã€‚

å¯¹äºç§æœ‰ GCS å­˜å‚¨æ¡¶ï¼Œè¯·å…ˆå¯¼å‡ºæ‚¨çš„æœåŠ¡è´¦æˆ·å¯†é’¥ã€‚å°†å…¶é‡å‘½åä¸º be å¹¶å°†å…¶æ·»åŠ åˆ° Truss çš„ç›®å½•ä¸­ã€‚service_account.jsondata

æ‚¨çš„æ–‡ä»¶ç»“æ„åº”å¦‚ä¸‹æ‰€ç¤ºï¼š


å¤åˆ¶

è¯¢é—® AI
your-truss
|--model
| â””â”€â”€ model.py
|--data
|. â””â”€â”€ service_account.json
å¦‚æœä½ å¯¹ Truss ä½¿ç”¨ç‰ˆæœ¬æ§åˆ¶ï¼ˆå¦‚ gitï¼‰ï¼Œè¯·ç¡®ä¿æ·»åŠ åˆ°ä½ çš„æ–‡ä»¶ä¸­ã€‚æ‚¨ä¸å¸Œæœ›æ„å¤–æš´éœ²æ‚¨çš„æœåŠ¡å¸æˆ·å¯†é’¥ã€‚service_account.json.gitignore

æƒé‡å°†ç¼“å­˜åœ¨ ã€‚/app/model_cache/{your_bucket_name}

â€‹
äºšé©¬é€Šäº‘ç§‘æŠ€ S3
å¦ä¸€ä¸ªç”¨äºæ‰˜ç®¡æ¨¡å‹æƒé‡çš„å¸¸ç”¨äº‘å­˜å‚¨é€‰é¡¹æ˜¯ AWS S3ï¼Œå°¤å…¶æ˜¯åœ¨æ‚¨å·²ç»åœ¨ä½¿ç”¨ AWS æœåŠ¡çš„æƒ…å†µä¸‹ã€‚

æ‚¨çš„åº”å¦‚ä¸‹æ‰€ç¤ºï¼šmodel_cache

config.yaml

å¤åˆ¶

è¯¢é—® AI
model_cache:
  - repo_id: s3://path-to-my-bucket
    use_volume: false
å¦‚æœæ‚¨æ­£åœ¨è®¿é—®å…¬æœ‰ S3 å­˜å‚¨æ¡¶ï¼Œåˆ™å¯ä»¥å¿½ç•¥åç»­æ­¥éª¤ï¼Œä½†è¯·ç¡®ä¿åœ¨å­˜å‚¨æ¡¶ä¸Šè®¾ç½®é€‚å½“çš„ç­–ç•¥ã€‚ç”¨æˆ·åº”è¯¥èƒ½å¤Ÿåˆ—å‡ºå’ŒæŸ¥çœ‹æ‰€æœ‰æ–‡ä»¶ã€‚å¦åˆ™ï¼Œæ¨¡å‹æ„å»ºå°†å¤±è´¥ã€‚

ä½†æ˜¯ï¼Œå¯¹äºç§æœ‰ S3 å­˜å‚¨æ¡¶ï¼Œæ‚¨éœ€è¦é¦–å…ˆåœ¨ AWS æ§åˆ¶é¢æ¿ä¸­æ‰¾åˆ° ã€ å’Œ ã€‚åˆ›å»ºåä¸º çš„æ–‡ä»¶ã€‚åœ¨æ­¤æ–‡ä»¶ä¸­ï¼Œæ·»åŠ æ‚¨ä¹‹å‰ç¡®å®šçš„å‡­è¯ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚å°†æ­¤æ–‡ä»¶æ”¾å…¥ Truss çš„ç›®å½•ä¸­ã€‚ é”®å¯ä»¥åŒ…å«åœ¨å†…ï¼Œä½†æ˜¯å¯é€‰çš„ã€‚aws_access_key_idaws_secret_access_keyaws_regions3_credentials.jsondataaws_session_token

ä»¥ä¸‹æ˜¯æ–‡ä»¶çš„å¤–è§‚ç¤ºä¾‹ï¼šs3_credentials.json


å¤åˆ¶

è¯¢é—® AI
{
    "aws_access_key_id": "YOUR-ACCESS-KEY",
    "aws_secret_access_key": "YOUR-SECRET-ACCESS-KEY",
    "aws_region": "YOUR-REGION"
}
æ‚¨çš„æ•´ä½“æ–‡ä»¶ç»“æ„ç°åœ¨åº”å¦‚ä¸‹æ‰€ç¤ºï¼š


å¤åˆ¶

è¯¢é—® AI
your-truss
|--model
| â””â”€â”€ model.py
|--data
|. â””â”€â”€ s3_credentials.json
åœ¨ç”Ÿæˆå‡­è¯æ—¶ï¼Œè¯·ç¡®ä¿ç”Ÿæˆçš„å¯†é’¥è‡³å°‘å…·æœ‰ä»¥ä¸‹ IAM ç­–ç•¥ï¼š


å¤åˆ¶

è¯¢é—® AI
{
        "Version": "2012-10-17",
        "Statement": [
            {
                "Action": [
                    "s3:GetObject",
                    "s3:ListObjects",
                ],
                "Effect": "Allow",
                "Resource": ["arn:aws:s3:::S3_BUCKET/PATH_TO_MODEL/*"]
            },
            {
                "Action": [
                    "s3:ListBucket",
                ],
                "Effect": "Allow",
                "Resource": ["arn:aws:s3:::S3_BUCKET"]
            }
        ]
    }
å¦‚æœä½ å¯¹ Truss ä½¿ç”¨ç‰ˆæœ¬æ§åˆ¶ï¼ˆå¦‚ gitï¼‰ï¼Œè¯·ç¡®ä¿æ·»åŠ åˆ°ä½ çš„æ–‡ä»¶ä¸­ã€‚æ‚¨ä¸å¸Œæœ›æ„å¤–æš´éœ²æ‚¨çš„æœåŠ¡å¸æˆ·å¯†é’¥ã€‚s3_credentials.json.gitignore

æƒé‡å°†ç¼“å­˜åœ¨ ã€‚/app/model_cache/{your_bucket_name}




é™ä½å†·å¯åŠ¨å»¶è¿Ÿçš„å®ç”¨æŒ‡å—ï¼šè®©ä½ çš„ Serverless åº”ç”¨é£èµ·æ¥
é™ä½å†·å¯åŠ¨å»¶è¿Ÿçš„å®ç”¨æŒ‡å—ï¼šè®©ä½ çš„ Serverless åº”ç”¨é£èµ·æ¥

1. ä»€ä¹ˆæ˜¯å†·å¯åŠ¨ï¼Ÿä¸ºä»€ä¹ˆå®ƒè®©äººå¤´ç–¼ï¼Ÿ

å¦‚æœä½ åˆšæ¥è§¦äº‘è®¡ç®—å’Œ Serverlessï¼Œå¯èƒ½ä¼šé‡åˆ°è¿™æ ·çš„é—®é¢˜ï¼šç”¨æˆ·ç¬¬ä¸€æ¬¡è®¿é—®ä½ çš„åº”ç”¨æ—¶ï¼Œå“åº”ç‰¹åˆ«æ…¢ï¼Œä½†åç»­è®¿é—®å°±å¾ˆå¿«ã€‚è¿™å°±æ˜¯"å†·å¯åŠ¨"åœ¨ä½œæ€ªã€‚

ç®€å•æ¥è¯´ï¼Œå†·å¯åŠ¨å°±åƒæ˜¯æ—©ä¸Šç¬¬ä¸€æ¬¡å¯åŠ¨æ±½è½¦ã€‚å¼•æ“æ˜¯å‡‰çš„ï¼Œéœ€è¦æ—¶é—´é¢„çƒ­æ‰èƒ½æ­£å¸¸å·¥ä½œã€‚åœ¨ Serverless ç¯å¢ƒä¸­ï¼Œå½“æ²¡æœ‰è¯·æ±‚æ—¶ï¼Œä½ çš„åº”ç”¨å®¹å™¨ä¼šè¢«"å†»ç»“"æˆ–é”€æ¯æ¥èŠ‚çœèµ„æºã€‚å½“æ–°è¯·æ±‚åˆ°æ¥æ—¶ï¼Œç³»ç»Ÿéœ€è¦é‡æ–°å¯åŠ¨å®¹å™¨ã€åŠ è½½ä»£ç ã€åˆå§‹åŒ–ç¯å¢ƒï¼Œè¿™ä¸ªè¿‡ç¨‹å°±æ˜¯å†·å¯åŠ¨ã€‚å¯¹äº AI æ¨¡å‹æ¨ç†åº”ç”¨æ¥è¯´ï¼Œå†·å¯åŠ¨æ›´åŠ æ˜æ˜¾ï¼Œå› ä¸ºè¿˜éœ€è¦é¢å¤–çš„æ—¶é—´æ¥åŠ è½½å‡ ä¸ª GB çš„æ¨¡å‹æƒé‡æ–‡ä»¶ã€‚

ä¸‹é¢è¿™ä¸ªå›¾æ¸…æ¥šåœ°å±•ç¤ºäº†å†·å¯åŠ¨çš„å®Œæ•´è¿‡ç¨‹ï¼š



å¯ä»¥çœ‹åˆ°ï¼Œçº¢è‰²çš„éƒ¨åˆ†éƒ½æ˜¯å†·å¯åŠ¨è¿‡ç¨‹ä¸­çš„è€—æ—¶æ­¥éª¤ï¼Œè€Œç»¿è‰²çš„éƒ¨åˆ†æ˜¯çƒ­å¯åŠ¨ï¼ˆå®¹å™¨å·²å­˜åœ¨ï¼‰çš„å¿«é€Ÿè·¯å¾„ã€‚

2. å†·å¯åŠ¨ä¼˜åŒ–ç­–ç•¥å¤§ç›˜ç‚¹

2.1 ä½¿ç”¨é¢„ç•™å¹¶å‘/ä¿æŒæ´»è·ƒ

æœ€ç›´æ¥çš„æ–¹æ³•å°±æ˜¯æå‰å‡†å¤‡å¥½"çƒ­"å®¹å™¨ï¼Œå°±åƒé¤å…åœ¨é«˜å³°æœŸå‰æå‰å‡†å¤‡å¥½é£Ÿæã€‚ä½ å¯ä»¥é€šè¿‡é…ç½® min_containers å’Œ buffer_containers å‚æ•°ï¼Œåœ¨é—²ç½®æœŸé—´ä¿æŒæœ€å°‘æ•°é‡çš„å®¹å™¨å¤„äº"æš–"çŠ¶æ€ï¼Œè¿˜å¯ä»¥é€šè¿‡ scaledown_window å»¶é•¿å®¹å™¨ç©ºé—²æ—¶é—´ã€‚è¿™æ ·åšçš„å¥½å¤„å¾ˆæ˜æ˜¾ï¼Œå®Œå…¨æ¶ˆé™¤é¢„ç•™å®ä¾‹çš„å†·å¯åŠ¨ï¼Œæä¾›ä¸€è‡´çš„ä½å»¶è¿Ÿå“åº”ï¼Œè¿˜æ”¯æŒåŠ¨æ€è°ƒæ•´æš–æ± å¤§å°åº”å¯¹é«˜å³°æœŸã€‚ä¸è¿‡ä»£ä»·ä¹Ÿå¾ˆæ˜æ˜¾ï¼Œæˆæœ¬ä¼šæ˜¾è‘—å¢åŠ ï¼Œå› ä¸ºéœ€è¦ä¸ºé—²ç½®èµ„æºä»˜è´¹ï¼Œè€Œä¸”éœ€è¦å‡†ç¡®é¢„ä¼°æµé‡æ¨¡å¼ã€‚

ä¸‹é¢è¿™ä¸ªæ¶æ„å›¾å±•ç¤ºäº†é¢„ç•™å¹¶å‘çš„å·¥ä½œåŸç†ï¼š



è¿™ä¸ªæ¶æ„çš„æ ¸å¿ƒæ˜¯ç»´æŠ¤ä¸€ä¸ªçƒ­å®¹å™¨æ± ï¼Œæ‰€æœ‰å®¹å™¨éƒ½å·²ç»é¢„çƒ­å®Œæˆï¼Œéšæ—¶å‡†å¤‡å¤„ç†è¯·æ±‚ã€‚

2.2 å»¶è¿ŸåŠ è½½æ¨¡å‹å’Œç¼“å­˜

å¦ä¸€ä¸ªèªæ˜çš„åšæ³•æ˜¯ä¸è¦åœ¨å¯åŠ¨æ—¶å°±åŠ è½½æ‰€æœ‰ä¸œè¥¿ï¼Œè€Œæ˜¯æŒ‰éœ€åŠ è½½å¹¶ç¼“å­˜ç»“æœã€‚å…·ä½“æ¥è¯´ï¼Œä½ å¯ä»¥ä»…åœ¨é¦–æ¬¡æ¨ç†è¯·æ±‚åˆ°è¾¾æ—¶æ‰åŠ è½½æ¨¡å‹æƒé‡ï¼Œä¸€æ—¦åŠ è½½å®Œæˆï¼Œå°±å°†å…¶ç¼“å­˜åœ¨å†…å­˜æˆ–æœ¬åœ°å­˜å‚¨ï¼ˆæ¯”å¦‚ SSDã€Network Volumeï¼‰ã€‚è¿™æ ·åšèƒ½å‡å°‘åˆå§‹å®¹å™¨å¯åŠ¨æ—¶é—´ï¼Œåç»­è¯·æ±‚ä¼šå—ç›Šäºç¼“å­˜ï¼Œè¿˜èƒ½é¿å…ä¸ºæœªä½¿ç”¨çš„æ¨¡å‹ä»˜è´¹ã€‚å½“ç„¶ï¼Œé¦–æ¬¡è¯·æ±‚ä»ç„¶ä¼šæœ‰æ¨¡å‹åŠ è½½å»¶è¿Ÿï¼Œè€Œä¸”éœ€è¦ç²¾å¿ƒè®¾è®¡åº”ç”¨æ¶æ„å’Œå­˜å‚¨ç­–ç•¥ã€‚

ä¸‹é¢è¿™ä¸ªæ—¶åºå›¾å±•ç¤ºäº†å»¶è¿ŸåŠ è½½å’Œç¼“å­˜çš„å·¥ä½œæµç¨‹ï¼š



å¯ä»¥çœ‹åˆ°ï¼Œè¿™ä¸ªç­–ç•¥çš„æ ¸å¿ƒæ˜¯å°†æ¨¡å‹åŠ è½½æ¨è¿Ÿåˆ°çœŸæ­£éœ€è¦çš„æ—¶å€™ï¼Œå¹¶é€šè¿‡ç¼“å­˜æ¥åŠ é€Ÿåç»­è¯·æ±‚ã€‚

2.3 é€šè¿‡å®šæ—¶ä»»åŠ¡æˆ–å¥åº·æ¢æµ‹é¢„çƒ­

è¿˜æœ‰ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„åŠæ³•å°±æ˜¯å®šæœŸ"å«é†’"ä½ çš„åº”ç”¨ï¼Œè®©å®ƒä¿æŒæ´»è·ƒçŠ¶æ€ã€‚ä½ å¯ä»¥å®æ–½å®šæ—¶"ping"ä»»åŠ¡ï¼ˆæ¯”å¦‚ cron ä½œä¸šï¼‰ï¼Œæˆ–è€…å‘é€åˆæˆè¯·æ±‚ä½œä¸ºå¥åº·æ£€æŸ¥ï¼Œå®šæœŸè°ƒç”¨æ¨ç†ç«¯ç‚¹ä»¥ä¿æŒå®¹å™¨æ´»è·ƒã€‚è¿™æ ·åšèƒ½ç¡®ä¿å®ä¾‹ä¿æŒæ´»è·ƒå’Œå‡†å¤‡å°±ç»ªï¼Œæœ‰æ•ˆé˜²æ­¢å†·å¯åŠ¨ï¼Œå®ç°æˆæœ¬ç›¸å¯¹è¾ƒä½ã€‚ä¸è¿‡ä¹Ÿæœ‰ä¸€äº›æˆæœ¬ï¼Œä¼šäº§ç”Ÿå°‘é‡ä½†æŒç»­çš„"ping"è¯·æ±‚è®¡ç®—æˆæœ¬ï¼Œå¯èƒ½éœ€è¦æ™ºèƒ½è°ƒåº¦é¿å…æµªè´¹ã€‚

3. å‰æ²¿æŠ€æœ¯è§£å†³æ–¹æ¡ˆ

3.1 Memory Snapshot å†…å­˜å¿«ç…§æŠ€æœ¯

ç°åœ¨æœ‰ä¸€äº›å¾ˆé…·çš„æ–°æŠ€æœ¯æ¥è§£å†³å†·å¯åŠ¨é—®é¢˜ã€‚Memory Snapshot å°±åƒæ¸¸æˆçš„"å¿«é€Ÿå­˜æ¡£"ï¼Œç›´æ¥æ¢å¤åˆ°é¢„çƒ­å®Œæˆçš„çŠ¶æ€ã€‚å…·ä½“åšæ³•æ˜¯åœ¨å®¹å™¨é¢„çƒ­å®Œæˆåæ•è·å†…å­˜çŠ¶æ€ï¼Œåç»­å†·å¯åŠ¨æ—¶ç›´æ¥æ¢å¤æ­¤å¿«ç…§ï¼Œè·³è¿‡å¤§éƒ¨åˆ†åˆå§‹åŒ–è¿‡ç¨‹ã€‚æ•ˆæœéå¸¸æ˜æ˜¾ï¼Œèƒ½å°†å†·å¯åŠ¨æ—¶é—´ä» 10-15 ç§’æ˜¾è‘—é™ä½è‡³ 3 ç§’ä»¥ä¸‹ï¼Œæœ€å¤šå¯å®ç° 3 å€æ€§èƒ½æå‡ã€‚ä¸è¿‡è¿™ä¸ªæŠ€æœ¯ä¹Ÿæœ‰é™åˆ¶ï¼Œåªèƒ½å¿«ç…§ CPU å†…å­˜ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç† GPU ç›¸å…³åˆå§‹åŒ–ï¼Œè€Œä¸”ä»£ç å¤æ‚åº¦ä¼šå¢åŠ ã€‚

ä¸‹é¢è¿™ä¸ªæµç¨‹å›¾å±•ç¤ºäº† Memory Snapshot çš„å·¥ä½œåŸç†ï¼š



è¿™ä¸ªæŠ€æœ¯çš„æ ¸å¿ƒæ˜¯å°†é¢„çƒ­å®Œæˆçš„å†…å­˜çŠ¶æ€"å†»ç»“"ä¿å­˜ï¼Œåç»­å¯åŠ¨æ—¶ç›´æ¥"è§£å†»"æ¢å¤ï¼Œè·³è¿‡è€—æ—¶çš„åˆå§‹åŒ–è¿‡ç¨‹ã€‚

3.2 FlashBoot ä¼˜åŒ–å±‚æŠ€æœ¯

FlashBoot æ˜¯å¦ä¸€ä¸ªå¾ˆæœ‰æ„æ€çš„æŠ€æœ¯ï¼Œé€šè¿‡é¢„æµ‹æ€§ç¼“å­˜å’Œä¼˜åŒ–çš„å®¹å™¨è°ƒåº¦å®ç°è¶…å¿«å†·å¯åŠ¨ã€‚è¿™ä¸ªæŠ€æœ¯çš„æ•ˆæœå¾ˆå‰å®³ï¼Œå†·å¯åŠ¨æ—¶é—´ä½è‡³ 500ms-1 ç§’ï¼Œ95% çš„å†·å¯åŠ¨åœ¨ 2.3 ç§’å†…å®Œæˆï¼Œè€Œä¸”æ— éœ€é¢å¤–è´¹ç”¨ã€‚ä¸è¿‡æ•ˆæœä¾èµ–äºç«¯ç‚¹çš„ä½¿ç”¨é¢‘ç‡å’Œæµé‡æ¨¡å¼ï¼Œå¯¹ä½é¢‘ä½¿ç”¨çš„ç«¯ç‚¹æ•ˆæœæœ‰é™ã€‚

3.3 åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿç¼“å­˜

åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿç¼“å­˜æ˜¯ä¸€ä¸ªå¾ˆå®ç”¨çš„æŠ€æœ¯ï¼ŒåŸºæœ¬æ€è·¯æ˜¯åœ¨åŒºåŸŸçº§åˆ«ç¼“å­˜æ¨¡å‹æƒé‡ï¼Œå¤šä¸ªå®ä¾‹å…±äº«ç¼“å­˜ã€‚ä½ å¯ä»¥å¯ç”¨åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿç¼“å­˜ï¼Œåœ¨åŒºåŸŸçº§åˆ«ç¼“å­˜æ¨¡å‹æƒé‡ï¼Œæ”¯æŒè·¨éƒ¨ç½²å…±äº«ç¼“å­˜ï¼Œ14 å¤©è‡ªåŠ¨åƒåœ¾å›æ”¶æœªä½¿ç”¨æ–‡ä»¶ã€‚æ•ˆæœå¾ˆæ˜æ˜¾ï¼Œç¼“å­˜å‘½ä¸­æ—¶è·³è¿‡ä¸‹è½½ï¼Œå®ç°æ¥è¿‘å³æ—¶çš„æƒé‡åŠ è½½ï¼Œå¤šä¸ª Pod é—´å¯çƒ­ç¼“å­˜å…±äº«ï¼Œæ˜¾è‘—æå‡ä¸‹è½½é€Ÿåº¦ï¼ˆ>1GB/sï¼‰ã€‚ä¸è¿‡ç›®å‰å¤„äº Beta é˜¶æ®µï¼Œéœ€è¦è”ç³»æ”¯æŒå¯ç”¨ï¼Œè€Œä¸”åˆæ¬¡ç¼“å­˜å¡«å……ä»éœ€ä»æºä¸‹è½½ã€‚

ä¸‹é¢è¿™ä¸ªæ¶æ„å›¾å±•ç¤ºäº†åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿç¼“å­˜çš„å·¥ä½œåŸç†ï¼š



è¿™ä¸ªæ¶æ„çš„å…³é”®æ˜¯åŒºåŸŸçº§çš„ç¼“å­˜å±‚ï¼Œæ‰€æœ‰ Pod éƒ½å¯ä»¥å…±äº«åŒä¸€ä»½ç¼“å­˜ï¼Œå¤§å¤§å‡å°‘äº†é‡å¤ä¸‹è½½çš„æ—¶é—´ã€‚

4. å¦‚ä½•é€‰æ‹©åˆé€‚çš„ä¼˜åŒ–ç­–ç•¥ï¼Ÿ

ä¸åŒçš„ä¼˜åŒ–ç­–ç•¥æœ‰ä¸åŒçš„ç‰¹ç‚¹å’Œé€‚ç”¨åœºæ™¯ã€‚ä¸‹é¢è¿™ä¸ªå¯¹æ¯”å›¾å¯ä»¥å¸®ä½ å¿«é€Ÿé€‰æ‹©æœ€åˆé€‚çš„ç­–ç•¥ï¼š



4.1 æ ¹æ®åº”ç”¨ç±»å‹é€‰æ‹©

ä¸åŒç±»å‹çš„åº”ç”¨éœ€è¦ä¸åŒçš„ä¼˜åŒ–ç­–ç•¥ã€‚å¦‚æœä½ çš„åº”ç”¨æ˜¯é«˜é¢‘è®¿é—®çš„ï¼Œæ¨èä½¿ç”¨é¢„ç•™å¹¶å‘/ä¿æŒæ´»è·ƒï¼Œè™½ç„¶æˆæœ¬è¾ƒé«˜ï¼Œä½†ç”¨æˆ·ä½“éªŒæœ€ä½³ã€‚ä¸­é¢‘è®¿é—®çš„åº”ç”¨å¯ä»¥è¯•è¯•å»¶è¿ŸåŠ è½½æ¨¡å‹å’Œç¼“å­˜åŠ ä¸Šå®šæ—¶é¢„çƒ­ï¼Œè¿™æ ·èƒ½å¹³è¡¡æˆæœ¬å’Œæ€§èƒ½ã€‚ä½é¢‘è®¿é—®çš„åº”ç”¨æ¨è Memory Snapshot æˆ– FlashBootï¼Œæ¥å—é¦–æ¬¡è®¿é—®ç¨æ…¢ï¼Œä½†åç»­è®¿é—®å¿«é€Ÿã€‚

4.2 æ ¹æ®é¢„ç®—é€‰æ‹©

é¢„ç®—å……è¶³çš„è¯ï¼Œä½¿ç”¨é¢„ç•™å¹¶å‘è·å¾—æœ€ä½³ç”¨æˆ·ä½“éªŒï¼Œç»“åˆåˆ†å¸ƒå¼ç¼“å­˜è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚é¢„ç®—æœ‰é™çš„è¯ï¼Œä¼˜å…ˆä½¿ç”¨å…è´¹çš„ FlashBoot æŠ€æœ¯ï¼Œé…åˆå®šæ—¶é¢„çƒ­å’Œå»¶è¿ŸåŠ è½½ã€‚

4.3 æ ¹æ®æŠ€æœ¯æ ˆé€‰æ‹©

AI æ¨¡å‹æ¨ç†åº”ç”¨å¿…é¡»ä½¿ç”¨æ¨¡å‹ç¼“å­˜å’Œé¢„å–ï¼Œç»“åˆå†…å­˜å¿«ç…§æŠ€æœ¯ã€‚ä¼ ç»Ÿ Web åº”ç”¨çš„è¯ï¼Œå®šæ—¶é¢„çƒ­åŠ ä¸Šå»¶è¿ŸåŠ è½½å°±å¤Ÿäº†ï¼Œè€ƒè™‘ä½¿ç”¨ FlashBoot ä¼˜åŒ–ã€‚

5. æœ€ä½³å®è·µå»ºè®®

5.1 ç›‘æ§å’Œæµ‹é‡

åœ¨ä¼˜åŒ–ä¹‹å‰ï¼Œå…ˆå»ºç«‹ç›‘æ§ç³»ç»Ÿï¼Œè®°å½•å†·å¯åŠ¨é¢‘ç‡å’Œå»¶è¿Ÿï¼Œç›‘æ§æˆæœ¬å˜åŒ–ï¼Œè®¾ç½®å‘Šè­¦é˜ˆå€¼ã€‚æ²¡æœ‰æ•°æ®å°±æ²¡æœ‰ä¼˜åŒ–çš„åŸºç¡€ã€‚

5.2 æ¸è¿›å¼ä¼˜åŒ–

ä¸è¦ä¸€æ¬¡æ€§åº”ç”¨æ‰€æœ‰ä¼˜åŒ–ç­–ç•¥ã€‚å…ˆä»å…è´¹çš„ FlashBoot å¼€å§‹ï¼Œæ ¹æ®ç›‘æ§æ•°æ®å†³å®šæ˜¯å¦éœ€è¦æ›´å¤šä¼˜åŒ–ï¼Œé€æ­¥å¼•å…¥æˆæœ¬è¾ƒé«˜çš„ç­–ç•¥ã€‚è¿™æ ·èƒ½é¿å…è¿‡åº¦ä¼˜åŒ–å’Œä¸å¿…è¦çš„æˆæœ¬æ”¯å‡ºã€‚

5.3 é¢„ä¼°æˆæœ¬

ä½¿ç”¨é¢„ç•™å¹¶å‘å‰ï¼Œä»”ç»†è®¡ç®—æˆæœ¬ã€‚è¯„ä¼°å®é™…æµé‡æ¨¡å¼ï¼Œè®¡ç®—é¢„ç•™èµ„æºçš„æˆæœ¬ï¼Œä¸å†·å¯åŠ¨å¯¼è‡´çš„ç”¨æˆ·æµå¤±æˆæœ¬å¯¹æ¯”ã€‚æœ‰æ—¶å€™å†·å¯åŠ¨çš„æˆæœ¬å¹¶ä¸å€¼å¾—ä¸ºäº†ä¼˜åŒ–è€Œä»˜å‡ºé«˜æ˜‚çš„é¢„ç•™æˆæœ¬ã€‚

5.4 å®šæœŸä¼˜åŒ–

å†·å¯åŠ¨ä¼˜åŒ–ä¸æ˜¯ä¸€æ¬¡æ€§å·¥ä½œã€‚å®šæœŸè¯„ä¼°ä¼˜åŒ–æ•ˆæœï¼Œæ ¹æ®ä¸šåŠ¡å¢é•¿è°ƒæ•´ç­–ç•¥ï¼Œå…³æ³¨æ–°æŠ€æœ¯å‘å±•ã€‚æŠ€æœ¯åœ¨ä¸æ–­è¿›æ­¥ï¼Œä¼˜åŒ–ç­–ç•¥ä¹Ÿè¦è·Ÿç€è°ƒæ•´ã€‚

6. æ€»ç»“

å†·å¯åŠ¨ä¼˜åŒ–æ˜¯ Serverless åº”ç”¨æ€§èƒ½ä¼˜åŒ–çš„é‡è¦ç¯èŠ‚ã€‚å¯¹äºåˆšå…¥é—¨çš„å¼€å‘è€…æ¥è¯´ï¼Œå»ºè®®ä»å…è´¹çš„ä¼˜åŒ–æŠ€æœ¯å¼€å§‹ï¼Œé€æ­¥æ·±å…¥ã€‚è®°ä½ï¼Œæœ€å¥½çš„ä¼˜åŒ–ç­–ç•¥æ˜¯é€‚åˆä½ çš„åº”ç”¨åœºæ™¯å’Œé¢„ç®—çš„ç­–ç•¥ã€‚

éšç€æŠ€æœ¯å‘å±•ï¼Œå†·å¯åŠ¨é—®é¢˜æ­£åœ¨é€æ­¥å¾—åˆ°è§£å†³ã€‚Memory Snapshotã€FlashBoot ç­‰æ–°æŠ€æœ¯è®©å†·å¯åŠ¨æ—¶é—´ä»å‡ åç§’é™ä½åˆ°å‡ ç§’ç”šè‡³å‡ ç™¾æ¯«ç§’ã€‚ç›¸ä¿¡åœ¨ä¸ä¹…çš„å°†æ¥ï¼Œå†·å¯åŠ¨å°†ä¸å†æ˜¯ Serverless åº”ç”¨çš„ç—›ç‚¹ã€‚



å‚è€ƒèµ„æ–™ï¼š
â€¢Modal æ–‡æ¡£ï¼šhttps://modal.com/docs/guide/cold-start#cold-start-performance
â€¢RunPod åšå®¢ï¼šhttps://blog.runpod.io/introducing-flashboot-1-second-serverless-cold-start/
â€¢Baseten æ–‡æ¡£ï¼šhttps://docs.baseten.co/development/model/model-cache#optimizing-access-time-futher-with-b10cache-enabled 







