# Cold start performance

Modal Functions are run in [containers](/docs/guide/images).

If a container is already ready to run your Function, it will be reused.

If not, Modal spins up a new container.
This is known as a _cold start_,
and it is often associated with higher latency.

There are two sources of increased latency during cold starts:

1. inputs may **spend more time waiting** in a queue for a container
   to become ready or "warm".
2. when an input is handled by the container that just started,
   there may be **extra work that only needs to be done on the first invocation**
   ("initialization").

This guide presents techniques and Modal features for reducing the impact of both queueing
and initialization on observed latencies.

If you are invoking Functions with no warm containers
or if you otherwise see inputs spending too much time in the "pending" state,
you should
[target queueing time for optimization](#reduce-time-spent-queueing-for-warm-containers).

If you see some Function invocations taking much longer than others,
and those invocations are the first handled by a new container,
you should
[target initialization for optimization](#reduce-latency-from-initialization).

## Reduce time spent queueing for warm containers

New containers are booted when there are not enough other warm containers to
to handle the current number of inputs.

For example, the first time you send an input to a Function,
there are zero warm containers and there is one input,
so a single container must be booted up.
The total latency for the input will include
the time it takes to boot a container.

If you send another input right after the first one finishes,
there will be one warm container and one pending input,
and no new container will be booted.

Generalizing, there are two factors that affect the time inputs spend queueing:
the time it takes for a container to boot and become warm (which we solve by booting faster)
and the time until a warm container is available to handle an input (which we solve by having more warm containers).

### Warm up containers faster

The time taken for a container to become warm
and ready for inputs can range from seconds to minutes.

Modal's custom container stack has been heavily optimized to reduce this time.
Containers boot in about one second.

But before a container is considered warm and ready to handle inputs,
we need to execute any logic in your code's global scope (such as imports)
or in any
[`modal.enter` methods](/docs/guide/lifecycle-functions).
So if your boots are slow, these are the first places to work on optimization.

For example, you might be downloading a large model from a model server
during the boot process.
You can instead
[download the model ahead of time](/docs/guide/model-weights),
so that it only needs to be downloaded once.

For models in the tens of gigabytes,
this can reduce boot times from minutes to seconds.

### Run more warm containers

It is not always possible to speed up boots sufficiently.
For example, seconds of added latency to load a model may not
be acceptable in an interactive setting.

In this case, the only option is to have more warm containers running.
This increases the chance that an input will be handled by a warm container,
for example one that finishes an input while another container is booting.

Modal currently exposes [three parameters](/docs/guide/scale) that control how
many containers will be warm: `scaledown_window`, `min_containers`,
and `buffer_containers`.

All of these strategies can increase the resources consumed by your Function
and so introduce a trade-off between cold start latencies and cost.

#### Keep containers warm for longer with `scaledown_window`

Modal containers will remain idle for a short period before shutting down. By
default, the maximum idle time is 60 seconds. You can configure this by setting
the `scaledown_window` on the [`@function`](/docs/reference/modal.App#function)
decorator. The value is measured in seconds, and it can be set anywhere between
two seconds and twenty minutes.

```python
import modal

app = modal.App()

@app.function(scaledown_window=300)
def my_idle_greeting():
    return {"hello": "world"}
```

Increasing the `scaledown_window` reduces the chance that subsequent requests
will require a cold start, although you will be billed for any resources used
while the container is idle (e.g., GPU reservation or residual memory
occupancy). Note that containers will not necessarily remain alive for the
entire window, as the autoscaler will scale down more agressively when the
Function is substantially over-provisioned.

#### Overprovision resources with `min_containers` and `buffer_containers`

Keeping already warm containers around longer doesn't help if there are no warm
containers to begin with, as when Functions scale from zero.

To keep some containers warm and running at all times, set the `min_containers`
value on the [`@function`](/docs/reference/modal.App#function) decorator. This
puts a floor on the the number of containers so that the Function doesn't scale
to zero. Modal will still scale up and spin down more containers as the
demand for your Function fluctuates above the `min_containers` value, as usual.

While `min_containers` overprovisions containers while the Function is idle,
`buffer_containers` provisions extra containers while the Function is active.
This "buffer" of extra containers will be idle and ready to handle inputs if
the rate of requests increases. This parameter is particularly useful for
bursty request patterns, where the arrival of one input predicts the arrival of more inputs,
like when a new user or client starts hitting the Function.

```python
import modal

app = modal.App(image=modal.Image.debian_slim().pip_install("fastapi"))

@app.function(min_containers=3, buffer_containers=3)
def my_warm_greeting():
    return "Hello, world!"
```

## Reduce latency from initialization

Some work is done the first time that a function is invoked
but can be used on every subsequent invocation.
This is
[_amortized work_](https://www.cs.cornell.edu/courses/cs312/2006sp/lectures/lec18.html)
done at initialization.

For example, you may be using a large pre-trained model
whose weights need to be loaded from disk to memory the first time it is used.

This results in longer latencies for the first invocation of a warm container,
which shows up in the application as occasional slow calls: high tail latency or elevated p9Xs.

### Move initialization work out of the first invocation

Some work done on the first invocation can be moved up and completed ahead of time.

Any work that can be saved to disk, like
[downloading model weights](/docs/guide/model-weights),
should be done as early as possible. The results can be included in the
[container's Image](/docs/guide/images)
or saved to a
[Modal Volume](/docs/guide/volumes).

Some work is tricky to serialize, like spinning up a network connection or an inference server.
If you can move this initialization logic out of the function body and into the global scope or a
[container `enter` method](https://modal.com/docs/guide/lifecycle-functions#enter),
you can move this work into the warm up period.
Containers will not be considered warm until all `enter` methods have completed,
so no inputs will be routed to containers that have yet to complete this initialization.

For more on how to use `enter` with machine learning model weights, see
[this guide](/docs/guide/model-weights).

Note that `enter` doesn't get rid of the latency --
it just moves the latency to the warm up period,
where it can be handled by
[running more warm containers](#run-more-warm-containers).

### Share initialization work across cold starts with memory snapshots

Cold starts can also be made faster by using memory snapshots.

Invocations of a Function after the first
are faster in part because the memory is already populated
with values that otherwise need to be computed or read from disk,
like the contents of imported libraries.

Memory snapshotting captures the state of a container's memory
at user-controlled points after it has been warmed up
and reuses that state in future boots, which can substantially
reduce cold start latency penalties and warm up period duration.

Refer to the [memory snapshot](/docs/guide/memory-snapshot)
guide for details.

### Optimize initialization code

Sometimes, there is nothing to be done but to speed this work up.

Here, we share specific patterns that show up in optimizing initialization
in Modal Functions.

#### Load multiple large files concurrently

Often Modal applications need to read large files into memory (eg. model
weights) before they can process inputs. Where feasible these large file
reads should happen concurrently and not sequentially. Concurrent IO takes
full advantage of our platform's high disk and network bandwidth
to reduce latency.

One common example of slow sequential IO is loading multiple independent
Huggingface `transformers` models in series.

```python notest
from transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForConditionalGeneration
model_a = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor_a = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
model_b = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
processor_b = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large")
```

The above snippet does four `.from_pretrained` loads sequentially.
None of the components depend on another being already loaded in memory, so they
can be loaded concurrently instead.

They could instead be loaded concurrently using a function like this:

```python notest
from concurrent.futures import ThreadPoolExecutor, as_completed
from transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForConditionalGeneration

def load_models_concurrently(load_functions_map: dict) -> dict:
    model_id_to_model = {}
    with ThreadPoolExecutor(max_workers=len(load_functions_map)) as executor:
        future_to_model_id = {
            executor.submit(load_fn): model_id
            for model_id, load_fn in load_functions_map.items()
        }
        for future in as_completed(future_to_model_id.keys()):
            model_id_to_model[future_to_model_id[future]] = future.result()
    return model_id_to_model

components = load_models_concurrently({
    "clip_model": lambda: CLIPModel.from_pretrained("openai/clip-vit-base-patch32"),
    "clip_processor": lambda: CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32"),
    "blip_model": lambda: BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large"),
    "blip_processor": lambda: BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large")
})
```

If performing concurrent IO on large file reads does _not_ speed up your cold
starts, it's possible that some part of your function's code is holding the
Python [GIL](https://wiki.python.org/moin/GlobalInterpreterLock) and reducing
the efficacy of the multi-threaded executor.







缓存权重 🆕
通过预取和缓存权重来加速冷启动和可用性。

​
什么是“冷启动”？
“冷启动”是一个术语，用于描述从模型缩放到 0 直到准备好处理第一个请求所花费的时间。此过程是使您的部署能够响应流量，同时保持 SLA 并降低成本的关键因素。 为了优化冷启动，我们将介绍以下策略：在模块导入期间运行的 Rust 后台线程中下载它们，在分布式文件系统中缓存权重，并将权重移动到 docker 镜像中。

在实践中，这会将大型模型的冷启动时间缩短到几秒钟。例如，Stable Diffusion XL 可能需要几分钟才能启动，而无需缓存。使用缓存时，只需不到 10 秒。

​
为模型启用缓存 + 预取
要启用缓存，只需将 .有几个关键配置：model_cacheconfig.yamlrepo_idmodel_cache

repo_id（必需）：来自 Hugging Face 的存储库名称。
revision（必需）：huggingface 存储库的修订版，例如 sha 或分支名称，例如 或 。refs/pr/1main
use_volume：布尔标志，用于确定权重是在运行时下载到 Baseten 分布式文件系统（推荐）还是捆绑到容器映像中（旧版，不推荐）。
volume_folder：字符串，模型权重显示在其下的文件夹名称。将其设置为 将在运行时将存储库挂载到。my-llama-model/app/model_cache/my-llama-model
allow_patterns：仅缓存与指定模式匹配的文件。使用 Unix shell 样式通配符来表示这些模式。
ignore_patterns：相反，您也可以表示要忽略的文件模式，从而简化缓存过程。
这是为 Stable Diffusion XL 编写的 Well 示例。请注意，它如何使用 .model_cacheallow_patterns

config.yaml

复制

询问 AI
model_cache:
  - repo_id: madebyollin/sdxl-vae-fp16-fix
    revision: 207b116dae70ace3637169f1ddd2434b91b3a8cd
    use_volume: true
    volume_folder: sdxl-vae-fp16
    allow_patterns:
      - config.json
      - diffusion_pytorch_model.safetensors
  - repo_id: stabilityai/stable-diffusion-xl-base-1.0
    revision: 462165984030d82259a11f4367a4eed129e94a7b
    use_volume: true
    volume_folder: stable-diffusion-xl-base
    allow_patterns:
      - "*.json"
      - "*.fp16.safetensors"
      - sd_xl_base_1.0.safetensors
  - repo_id: stabilityai/stable-diffusion-xl-refiner-1.0
    revision: 5d4cfe854c9a9a87939ff3653551c2b3c99a4356
    use_volume: true
    volume_folder: stable-diffusion-xl-refiner
    allow_patterns:
      - "*.json"
      - "*.fp16.safetensors"
      - sd_xl_refiner_1.0.safetensors
许多 Hugging Face 存储库具有不同格式（、、、 等）的模型权重。大多数时候，您只需要其中之一。为了最大限度地减少冷启动，请确保仅缓存所需的权重。.bin.safetensors.h5.msgpack

​
什么是权重“预取”？
使用 ，权重是通过在专用的 Rust 线程中提前下载权重来预取的。 这意味着，您可以执行各种准备工作（导入库、torch/triton 模块的 jit 编译），直到您需要访问这些文件为止。 在实践中，执行这样的语句通常需要 10-15 秒。届时，前 5-10GB 的权重已经下载完毕。model_cacheimport tensorrt_llm

要将配置与 truss 一起使用，我们要求您主动与 . 在使用任何下载的文件之前，您必须调用 .这将阻止，直到目录中的所有文件都下载并准备好使用。 此调用必须是 your 或 implementation 的一部分。model_cachelazy_data_resolverlazy_data_resolver.block_until_download_complete()/app/model_cache__init__load

model.py

复制

询问 AI
# <- download is invoked before here.
import torch # this line usually takes 2-5 seconds.
import tensorrt_llm # this line usually takes 10-15 seconds
import onnxruntime # this line usually takes 5-10 seconds

class Model:
    """example usage of `model_cache` in truss"""
    def __init__(self, *args, **kwargs):
        # `lazy_data_resolver` is passed as keyword-argument in init
        self._lazy_data_resolver = kwargs["lazy_data_resolver"]

    def load():
        # work that does not require the download may be done beforehand
        random_vector = torch.randn(1000)
        # important to collect the download before using any incomplete data
        self._lazy_data_resolver.block_until_download_complete()
        # after the call, you may use the /app/model_cache directory and the contents
        torch.load(
            "/app/model_cache/stable-diffusion-xl-base/model.fp16.safetensors"
        )
​
私有 Hugging Face 存储库 🤗
对于任何公共 Hugging Face 存储库，您无需执行任何其他作。添加带有 appropriate 的密钥应该就足够了。model_cacherepo_id

但是，如果您想将模型从 Llama 2 等门控仓库部署到 Baseten，则需要采取以下几个步骤：

1
获取 Hugging Face API 密钥

从 Hugging Face 获取具有访问权限的 API 密钥。确保您有权访问要服务的模型。read

2
将其添加到 Baseten Secrets Manager

将您的 API 密钥粘贴到 Baseten 中密钥管理器的密钥 .您可以在此处阅读有关 secret 的更多信息。hf_access_token

3
更新配置

在 Truss's 中，添加以下代码：config.yaml

config.yaml

复制

询问 AI
secrets:
  hf_access_token: null
确保密钥在您的 .secretsconfig.yaml

如果您遇到任何问题，请再次执行上述所有步骤，并确保您没有拼错存储库的名称或粘贴不正确的 API 密钥。

​
model_cache在 Chains 中
要用于链 - 请使用说明符。在下面的示例中，我们将下载 . 由于此模型是封闭式 huggingface 模型，因此我们将挂载令牌设置为资产的一部分。 该模型非常小 - 在许多情况下，我们将能够在运行时下载模型。model_cacheAssetsllama-3.2-1Bchains.Assets(..., secret_keys=["hf_access_token"])from transformers import pipelineimport torch

chain_cache.py

复制

询问 AI
import random
import truss_chains as chains

try:
    # imports on global level for PoemGeneratorLM, to save time during the download.
    from transformers import pipeline
    import torch
except ImportError:
    # RandInt does not have these dependencies.
    pass

class RandInt(chains.ChainletBase):
    async def run_remote(self, max_value: int) -> int:
        return random.randint(1, max_value)

@chains.mark_entrypoint
class PoemGeneratorLM(chains.ChainletBase):
    from truss import truss_config
    LLAMA_CACHE = truss_config.ModelRepo(
        repo_id="meta-llama/Llama-3.2-1B-Instruct",
        revision="c4219cc9e642e492fd0219283fa3c674804bb8ed",
        use_volume=True,
        volume_folder="llama_mini",
        ignore_patterns=["*.pth", "*.onnx"]
    )
    remote_config = chains.RemoteConfig(
        docker_image=chains.DockerImage(
            # The phi model needs some extra python packages.
            pip_requirements=[
                "transformers==4.48.0",
                "torch==2.6.0",
            ]
        ),
        compute=chains.Compute(
            gpu="L4"
        ),
        # The phi model needs a GPU and more CPUs.
        # compute=chains.Compute(cpu_count=2, gpu="T4"),
        # Cache the model weights in the image
        assets=chains.Assets(cached=[LLAMA_CACHE], secret_keys=["hf_access_token"]),
    )
    # <- Download happens before __init__ is called.
    def __init__(self, rand_int=chains.depends(RandInt, retries=3)) -> None:
        self._rand_int = rand_int
        print("loading cached llama_mini model")
        self.pipeline = pipeline(
            "text-generation",
            model=f"/app/model_cache/llama_mini",
        )

    async def run_remote(self, max_value: int = 3) -> str:
        num_repetitions = await self._rand_int.run_remote(max_value)
        print("writing poem with num_repetitions", num_repetitions)
        poem = str(self.pipeline(
            text_inputs="Write a beautiful and descriptive poem about the ocean. Focus on its vastness, movement, and colors.",
            max_new_tokens=150,
            do_sample=True,
            return_full_text=False,
            temperature=0.7,
            top_p=0.9,
        )[0]['generated_text'])
        return poem * num_repetitions
​
model_cache用于自定义服务器
如果您不使用 Python 和自定义服务器，例如 vllm、TEI 或 sglang， 您需要使用命令 来强制填充该位置。该命令将阻止，直到下载权重。model.pytruss-transfer-cli/app/model_cache

下面是一个示例，说明如何在 L4 上使用 text-embeddings-inference 将 jina 嵌入模型从 huggingface 填充到 model_cache 中。

config.yaml

复制

询问 AI
base_image:
  image: baseten/text-embeddings-inference-mirror:89-1.6
docker_server:
  liveness_endpoint: /health
  predict_endpoint: /v1/embeddings
  readiness_endpoint: /health
  server_port: 7997
  # using `truss-transfer-cli` to download the weights to `cached_model`
  start_command: bash -c "truss-transfer-cli && text-embeddings-router --port 7997
    --model-id /app/model_cache/my_jina --max-client-batch-size 128 --max-concurrent-requests
    128 --max-batch-tokens 16384 --auto-truncate"
model_cache:
- repo_id: jinaai/jina-embeddings-v2-base-code
  revision: 516f4baf13dec4ddddda8631e019b5737c8bc250
  use_volume: true
  volume_folder: my_jina
  ignore_patterns: ["*.onnx"]
model_metadata:
  example_model_input:
    encoding_format: float
    input: text string
    model: model
model_name: TEI-jinaai-jina-embeddings-v2-base-code-truss-example
resources:
  accelerator: L4
​
在启用 b10cache 的情况下进一步优化访问时间
B10Cache 目前处于 Beta 版
为了进一步减少权重加载时间，我们可以为您的账户启用 Baseten 的分布式文件系统 （b10cache）。 您可以通过查看部署日志来验证是否已为您的账户启用此功能。


复制

询问 AI
[2025-09-10 01:04:35] [INFO ] b10cache is enabled.
[2025-09-10 01:04:35] [INFO ] Symlink created successfully. Skipping download for /app/model_cache/cached_model/model.safetensors
一旦 b10cache 处于活动状态，我们将跳过缓存在运行部署的区域的分布式文件系统中的下载。 b10cache 就像一个内容分发网络：初始缓存未命中会填充文件系统，未使用的文件会在上次使用 4 天后进行垃圾回收。 一旦 b10cache 处于活动状态，它将从最快的源拉取。如果另一个 Pod 在同一物理节点上处于活动状态，则构件可能会被热缓存，并在您的部署之间共享。 下载与其他组织完全隔离。不建议就地/不复制修改下载的构件。

如果 b10cache 不适用于您的账户，我们将为model_cache提供来自 HuggingFace.co 的优化下载。 下载是并行的，在 10Gbit 以太网连接上实现超过 1GB/s 的典型下载速度。 如果您想启用 b10cache，请随时联系我们的支持人员。

​
旧版缓存 - 容器中的权重
确保权重始终可用的一种较慢的方法是在构建时将它们下载到 docker 镜像中。 我们仅建议对最大 ~1GB 的小型模型执行此作。

权衡：

最高可用性：模型权重永远不会依赖于 S3/huggingface 正常运行时间。b10cache 上的高可用性。
较慢的冷启动：可能需要从速度较慢的 S3 或 Huggingface 速度较慢的源中提取 Docker 映像。
不适合非常大的模型：我们不建议将大型模型构件放入 docker 镜像中，当大于 50GB 时，可能会导致构建失败。
​
通过以下方式将权重下载到图像中build_commands
将权重下载到 docker 映像的最灵活方法是使用 custom 。 您可以在此处阅读有关 build_commands 的更多信息。build_commands

config.yaml

复制

询问 AI
build_commands:
- 'apt-get install git git-lfs'
- 'git lfs install'
- 'git clone https://huggingface.co/nomic-ai/nomic-embed-text-v1.5 /data/local-model'
- echo 'Model downloaded to /data/local-model via git clone'
​
通过 和 下载权重model_cacheuse_volume: false
如果您设置 ，我们将不会在运行时使用 b10cache 来挂载模型权重，而是将它们供应商到 docker 镜像中。use_volume: false

​
拥抱脸
config.yaml

复制

询问 AI
model_cache:
  - repo_id: madebyollin/sdxl-vae-fp16-fix
    revision: 207b116dae70ace3637169f1ddd2434b91b3a8cd
    use_volume: false
    allow_patterns:
      - config.json
      - diffusion_pytorch_model.safetensors
权重将缓存在默认的 Hugging Face 缓存目录 .您可以通过在 .~/.cache/huggingface/hub/models--{your_model_name}/HF_HOMEHUGGINGFACE_HUB_CACHEconfig.yaml

在此处阅读更多内容。

Huggingface 库将直接使用它。

model.py

复制

询问 AI
from transformers import AutoModel

AutoModel.from_pretrained("madebyollin/sdxl-vae-fp16-fix")
​
Google 云存储
当您拥有自定义模型或要进行微调时，Google Cloud Storage 是 Hugging Face 的绝佳替代品，特别是如果您已经在使用 GCP 并关心安全性和合规性。

您的应如下所示：model_cache

config.yaml

复制

询问 AI
model_cache:
  - repo_id: gs://path-to-my-bucket
    use_volume: false
如果您正在访问公有 GCS 存储桶，则可以忽略以下步骤，但请确保在存储桶上设置适当的权限。用户应该能够列出和查看所有文件。否则，模型构建将失败。

对于私有 GCS 存储桶，请先导出您的服务账户密钥。将其重命名为 be 并将其添加到 Truss 的目录中。service_account.jsondata

您的文件结构应如下所示：


复制

询问 AI
your-truss
|--model
| └── model.py
|--data
|. └── service_account.json
如果你对 Truss 使用版本控制（如 git），请确保添加到你的文件中。您不希望意外暴露您的服务帐户密钥。service_account.json.gitignore

权重将缓存在 。/app/model_cache/{your_bucket_name}

​
亚马逊云科技 S3
另一个用于托管模型权重的常用云存储选项是 AWS S3，尤其是在您已经在使用 AWS 服务的情况下。

您的应如下所示：model_cache

config.yaml

复制

询问 AI
model_cache:
  - repo_id: s3://path-to-my-bucket
    use_volume: false
如果您正在访问公有 S3 存储桶，则可以忽略后续步骤，但请确保在存储桶上设置适当的策略。用户应该能够列出和查看所有文件。否则，模型构建将失败。

但是，对于私有 S3 存储桶，您需要首先在 AWS 控制面板中找到 、 和 。创建名为 的文件。在此文件中，添加您之前确定的凭证，如下所示。将此文件放入 Truss 的目录中。 键可以包含在内，但是可选的。aws_access_key_idaws_secret_access_keyaws_regions3_credentials.jsondataaws_session_token

以下是文件的外观示例：s3_credentials.json


复制

询问 AI
{
    "aws_access_key_id": "YOUR-ACCESS-KEY",
    "aws_secret_access_key": "YOUR-SECRET-ACCESS-KEY",
    "aws_region": "YOUR-REGION"
}
您的整体文件结构现在应如下所示：


复制

询问 AI
your-truss
|--model
| └── model.py
|--data
|. └── s3_credentials.json
在生成凭证时，请确保生成的密钥至少具有以下 IAM 策略：


复制

询问 AI
{
        "Version": "2012-10-17",
        "Statement": [
            {
                "Action": [
                    "s3:GetObject",
                    "s3:ListObjects",
                ],
                "Effect": "Allow",
                "Resource": ["arn:aws:s3:::S3_BUCKET/PATH_TO_MODEL/*"]
            },
            {
                "Action": [
                    "s3:ListBucket",
                ],
                "Effect": "Allow",
                "Resource": ["arn:aws:s3:::S3_BUCKET"]
            }
        ]
    }
如果你对 Truss 使用版本控制（如 git），请确保添加到你的文件中。您不希望意外暴露您的服务帐户密钥。s3_credentials.json.gitignore

权重将缓存在 。/app/model_cache/{your_bucket_name}




降低冷启动延迟的实用指南：让你的 Serverless 应用飞起来
降低冷启动延迟的实用指南：让你的 Serverless 应用飞起来

1. 什么是冷启动？为什么它让人头疼？

如果你刚接触云计算和 Serverless，可能会遇到这样的问题：用户第一次访问你的应用时，响应特别慢，但后续访问就很快。这就是"冷启动"在作怪。

简单来说，冷启动就像是早上第一次启动汽车。引擎是凉的，需要时间预热才能正常工作。在 Serverless 环境中，当没有请求时，你的应用容器会被"冻结"或销毁来节省资源。当新请求到来时，系统需要重新启动容器、加载代码、初始化环境，这个过程就是冷启动。对于 AI 模型推理应用来说，冷启动更加明显，因为还需要额外的时间来加载几个 GB 的模型权重文件。

下面这个图清楚地展示了冷启动的完整过程：



可以看到，红色的部分都是冷启动过程中的耗时步骤，而绿色的部分是热启动（容器已存在）的快速路径。

2. 冷启动优化策略大盘点

2.1 使用预留并发/保持活跃

最直接的方法就是提前准备好"热"容器，就像餐厅在高峰期前提前准备好食材。你可以通过配置 min_containers 和 buffer_containers 参数，在闲置期间保持最少数量的容器处于"暖"状态，还可以通过 scaledown_window 延长容器空闲时间。这样做的好处很明显，完全消除预留实例的冷启动，提供一致的低延迟响应，还支持动态调整暖池大小应对高峰期。不过代价也很明显，成本会显著增加，因为需要为闲置资源付费，而且需要准确预估流量模式。

下面这个架构图展示了预留并发的工作原理：



这个架构的核心是维护一个热容器池，所有容器都已经预热完成，随时准备处理请求。

2.2 延迟加载模型和缓存

另一个聪明的做法是不要在启动时就加载所有东西，而是按需加载并缓存结果。具体来说，你可以仅在首次推理请求到达时才加载模型权重，一旦加载完成，就将其缓存在内存或本地存储（比如 SSD、Network Volume）。这样做能减少初始容器启动时间，后续请求会受益于缓存，还能避免为未使用的模型付费。当然，首次请求仍然会有模型加载延迟，而且需要精心设计应用架构和存储策略。

下面这个时序图展示了延迟加载和缓存的工作流程：



可以看到，这个策略的核心是将模型加载推迟到真正需要的时候，并通过缓存来加速后续请求。

2.3 通过定时任务或健康探测预热

还有一个简单有效的办法就是定期"叫醒"你的应用，让它保持活跃状态。你可以实施定时"ping"任务（比如 cron 作业），或者发送合成请求作为健康检查，定期调用推理端点以保持容器活跃。这样做能确保实例保持活跃和准备就绪，有效防止冷启动，实现成本相对较低。不过也有一些成本，会产生少量但持续的"ping"请求计算成本，可能需要智能调度避免浪费。

3. 前沿技术解决方案

3.1 Memory Snapshot 内存快照技术

现在有一些很酷的新技术来解决冷启动问题。Memory Snapshot 就像游戏的"快速存档"，直接恢复到预热完成的状态。具体做法是在容器预热完成后捕获内存状态，后续冷启动时直接恢复此快照，跳过大部分初始化过程。效果非常明显，能将冷启动时间从 10-15 秒显著降低至 3 秒以下，最多可实现 3 倍性能提升。不过这个技术也有限制，只能快照 CPU 内存，需要特殊处理 GPU 相关初始化，而且代码复杂度会增加。

下面这个流程图展示了 Memory Snapshot 的工作原理：



这个技术的核心是将预热完成的内存状态"冻结"保存，后续启动时直接"解冻"恢复，跳过耗时的初始化过程。

3.2 FlashBoot 优化层技术

FlashBoot 是另一个很有意思的技术，通过预测性缓存和优化的容器调度实现超快冷启动。这个技术的效果很厉害，冷启动时间低至 500ms-1 秒，95% 的冷启动在 2.3 秒内完成，而且无需额外费用。不过效果依赖于端点的使用频率和流量模式，对低频使用的端点效果有限。

3.3 分布式文件系统缓存

分布式文件系统缓存是一个很实用的技术，基本思路是在区域级别缓存模型权重，多个实例共享缓存。你可以启用分布式文件系统缓存，在区域级别缓存模型权重，支持跨部署共享缓存，14 天自动垃圾回收未使用文件。效果很明显，缓存命中时跳过下载，实现接近即时的权重加载，多个 Pod 间可热缓存共享，显著提升下载速度（>1GB/s）。不过目前处于 Beta 阶段，需要联系支持启用，而且初次缓存填充仍需从源下载。

下面这个架构图展示了分布式文件系统缓存的工作原理：



这个架构的关键是区域级的缓存层，所有 Pod 都可以共享同一份缓存，大大减少了重复下载的时间。

4. 如何选择合适的优化策略？

不同的优化策略有不同的特点和适用场景。下面这个对比图可以帮你快速选择最合适的策略：



4.1 根据应用类型选择

不同类型的应用需要不同的优化策略。如果你的应用是高频访问的，推荐使用预留并发/保持活跃，虽然成本较高，但用户体验最佳。中频访问的应用可以试试延迟加载模型和缓存加上定时预热，这样能平衡成本和性能。低频访问的应用推荐 Memory Snapshot 或 FlashBoot，接受首次访问稍慢，但后续访问快速。

4.2 根据预算选择

预算充足的话，使用预留并发获得最佳用户体验，结合分布式缓存进一步优化。预算有限的话，优先使用免费的 FlashBoot 技术，配合定时预热和延迟加载。

4.3 根据技术栈选择

AI 模型推理应用必须使用模型缓存和预取，结合内存快照技术。传统 Web 应用的话，定时预热加上延迟加载就够了，考虑使用 FlashBoot 优化。

5. 最佳实践建议

5.1 监控和测量

在优化之前，先建立监控系统，记录冷启动频率和延迟，监控成本变化，设置告警阈值。没有数据就没有优化的基础。

5.2 渐进式优化

不要一次性应用所有优化策略。先从免费的 FlashBoot 开始，根据监控数据决定是否需要更多优化，逐步引入成本较高的策略。这样能避免过度优化和不必要的成本支出。

5.3 预估成本

使用预留并发前，仔细计算成本。评估实际流量模式，计算预留资源的成本，与冷启动导致的用户流失成本对比。有时候冷启动的成本并不值得为了优化而付出高昂的预留成本。

5.4 定期优化

冷启动优化不是一次性工作。定期评估优化效果，根据业务增长调整策略，关注新技术发展。技术在不断进步，优化策略也要跟着调整。

6. 总结

冷启动优化是 Serverless 应用性能优化的重要环节。对于刚入门的开发者来说，建议从免费的优化技术开始，逐步深入。记住，最好的优化策略是适合你的应用场景和预算的策略。

随着技术发展，冷启动问题正在逐步得到解决。Memory Snapshot、FlashBoot 等新技术让冷启动时间从几十秒降低到几秒甚至几百毫秒。相信在不久的将来，冷启动将不再是 Serverless 应用的痛点。



参考资料：
•Modal 文档：https://modal.com/docs/guide/cold-start#cold-start-performance
•RunPod 博客：https://blog.runpod.io/introducing-flashboot-1-second-serverless-cold-start/
•Baseten 文档：https://docs.baseten.co/development/model/model-cache#optimizing-access-time-futher-with-b10cache-enabled 







