---
pubDate: 2024-07-24
description: 深入探讨主成分分析（PCA）与K-Means聚类的理论与实践，通过从零实现和3D/2D可视化，直观地理解其如何在高维数据中寻找关键特征并实现降维与聚类。
---

# 第 2 章. 聚类与降维 (Clustering and Dimensionality Reduction)

在处理复杂的机器学习问题时，一个关键决策是选择哪些特征来训练模型。过多的特征不仅会增加计算负担，还可能引入噪声，影响模型性能。降维技术，特别是主成分分析（PCA），允许我们识别出数据中最重要的模式，将数据集简化为少数几个核心变量，同时最大限度地保留原始信息。

## 2.1 主成分分析 (PCA)：数据的降维艺术

PCA 的核心思想是在高维数据中找到一组新的坐标轴（即“主成分”），使得数据在这些轴上的方差最大。第一个主成分捕捉了数据中最大的变异性，第二个主成分在与第一个正交的前提下捕捉剩余变异性的最大部分，以此类推。

### 2.1.1 任务设定：在高维数据中寻找核心方向

为了直观地理解 PCA，我们首先生成一个三维的“带状”数据集，这些数据点大致分布在一个平面上，并带有一些随机噪声。我们的目标就是利用 PCA 发现这个隐藏的主要平面。

```python
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import scienceplots
from celluloid import Camera
from IPython.display import Image

np.random.seed(0)
plt.style.use(["science", "no-latex"])

def generate_noisy_hyperplane(num_points, start_pt, end_pt, noise=0.25):
    """在两点之间生成带噪声的平面数据"""
    t = np.linspace(0.0 + noise, 1.0 - noise, num_points).reshape(-1, 1)
    points = start_pt + t * (end_pt - start_pt)
    noise = np.random.normal(0, noise, size=(num_points, 3))
    points = points + noise
    return points

# 生成数据
start_pt = np.array([-1, -1, -1])
end_pt = np.array([1, 1, 1])
X = generate_noisy_hyperplane(200, start_pt, end_pt)

# 绘制原始数据
fig = plt.figure()
ax = fig.add_subplot(111, projection="3d")
ax.scatter(X[:, 0], X[:, 1], X[:, 2], alpha=0.2, color="blue", label="Original Data")
ax.set_title("Original 3D Data")
plt.show()
```

### 2.1.2 核心理论：协方差矩阵与特征分解

PCA 的数学推导非常优雅。其核心目标是找到一个方向向量 `u`，使得数据投影到这个方向上的方差最大。

给定中心化后的数据矩阵 `X`，数据点 `x_i` 在 `u` 上的投影为 `x_i^T u`。所有点的投影方差为：
$$
\text{Var}(X u) = \frac{1}{n-1} \sum_{i=1}^{n} (x_i^T u)^2 = u^T \left( \frac{1}{n-1} X^T X \right) u = u^T C u
$$
其中 `C` 是 `X` 的协方差矩阵。

我们的问题就变成了一个带约束的优化问题：
- **最大化**: $u^T C u$
- **约束**: $u^T u = 1$ (因为我们只关心方向，所以要求 `u` 是单位向量)

使用拉格朗日乘子法可以求解这个问题。我们构造拉格朗日函数：
$$
L(u, \lambda) = u^T C u - \lambda(u^T u - 1)
$$
对其求关于 `u` 的梯度并令其为零：
$$
\frac{\partial L}{\partial u} = 2Cu - 2\lambda u = 0 \implies Cu = \lambda u
$$
这个方程正是**特征值分解**的定义！

这意味着：
-   我们要找的**主成分（方向向量 `u`）**，正是数据协方差矩阵 `C` 的**特征向量**。
-   每个主成分所能解释的**方差大小**，正是其对应的**特征值 `λ`**。

因此，PCA 的算法本质就是对数据的协方差矩阵进行特征分解。

### 2.1.3 代码实现：从零构建 PCA

根据上述理论，我们可以将 PCA 的实现步骤总结如下：

1.  **标准化数据**：将数据中心化（减去均值），并可选地进行缩放（除以标准差）。
2.  **计算协方差矩阵**。
3.  **特征分解**：计算协方差矩阵的特征值和特征向量。
4.  **排序**：根据特征值从大到小对特征向量进行排序。
5.  **投影**：将标准化后的数据乘以排序后的特征向量矩阵，得到降维后的新数据。

```python
def pca(X, dims):
    """从零实现的PCA算法"""
    # 1. 标准化数据
    X_centered = (X - np.mean(X, axis=0)) / np.std(X, axis=0)

    # 2. 计算协方差矩阵
    cov = np.cov(X_centered.T)

    # 3. 特征分解
    eig_vals, eig_vecs = np.linalg.eig(cov)

    # 4. 排序特征值和特征向量
    sorted_idx = np.argsort(eig_vals)[::-1]
    eig_vals = eig_vals[sorted_idx]
    eig_vecs = eig_vecs[:, sorted_idx]

    # 5. 投影数据
    projected = X_centered @ eig_vecs

    # 计算每个主成分的方差
    pc_variances = [np.var(projected[:, i]) for i in range(dims)]

    return eig_vals, eig_vecs, projected, pc_variances
```

### 2.1.4 可视化：观察投影过程

为了更直观地展示 PCA 的效果，我们创建一系列辅助函数来绘图，并最终通过一个主函数 `visualize_pca` 来生成动画，逐步将原始数据投影到由主成分定义的超平面上。

```python
def create_plots_pca():
    """创建用于PCA可视化的画布和子图"""
    fig = plt.figure(figsize=(16 / 9.0 * 4, 4 * 1))
    fig.suptitle("Principal Component Analysis")

    ax0 = fig.add_subplot(121, projection="3d")
    ax0.set_xlabel("X")
    ax0.set_ylabel("Y")
    ax0.set_zlabel("Z")
    ax0.set_title("PC Hyperplanes")
    ax0.view_init(17, -125, 2)
    ax0.set_xlim(-1, 1)
    ax0.set_ylim(-1, 1)
    ax0.set_zlim(-1, 1)
    ax0.tick_params(axis="both", which="both", length=0)

    ax1 = fig.add_subplot(122, projection="3d")
    ax1.set_xlabel("X")
    ax1.set_ylabel("Y")
    ax1.set_zlabel("Z")
    ax1.set_title("Projected Data")
    ax1.view_init(17, -125, 2)
    ax1.tick_params(axis="both", which="both", length=0)

    camera = Camera(fig)
    return ax0, ax1, camera

def plot_hyperplane(ax, pc_vector, color="red", scaling=10, alpha=0.3):
    """根据主成分向量绘制超平面"""
    points = np.linspace(-1, 1, scaling)
    xx, yy = np.meshgrid(points, points)

    # 标准化主成分向量
    pc_vector /= np.linalg.norm(pc_vector)
    # 根据平面方程计算z值
    z = (-pc_vector[0] * xx - pc_vector[1] * yy) / pc_vector[2]

    ax.plot_surface(xx, yy, z, color=color, alpha=alpha)

def visualize_pca(X, dims, output_filename):
    """可视化PCA降维过程"""
    ax0, ax1, camera = create_plots_pca()
    colors = ["red", "green", "blue"]

    for dim in range(0, dims + 1):
        eig_vals, eig_vecs, projected, pc_variances = pca(X, dims)

        # 绘制原始数据
        ax0.scatter(X[:, 0], X[:, 1], X[:, 2], color="blue", label="Original Data")

        # 绘制主成分超平面
        for i in range(dim):
            plot_hyperplane(ax0, eig_vecs[:, i], color=colors[i])

        # 绘制投影后的数据
        curr_projected = projected.copy()
        # 将非主成分维度清零
        for i in range(dim, dims):
            if i < dims:
                curr_projected[:, i] = 0
        if dim != 0:
            ax1.scatter(
                curr_projected[:, 0], curr_projected[:, 1], curr_projected[:, 2],
                color="blue", label="Projected Data"
            )
        camera.snap()

    animation = camera.animate(interval=2000)
    animation.save(output_filename, writer="pillow")
    plt.show()

dims = 3
output_filename = "pca.gif"
visualize_pca(X, dims, output_filename)
```

动画 `pca.gif` 展示了：
-   **左图**: 原始数据点以及依次出现的主成分超平面（第一主成分为红色，第二为绿色等）。
-   **右图**: 原始数据被投影到这些主成分超平面上的结果。可以看到，当只投影到第一个主成分（红色平面）上时，数据点已经很好地展现了其主要分布趋势。

![PCA Animation](../_images/ebaad0f4ac4feb5c2c254491808b34a4999f7bfbd46d0f6c7c0ec235d9697300.gif)

**结果分析**
```
variance percentage per principal component
1th PC: 69.12%
2th PC: 17.52%
3th PC: 13.36%

variance per principal component
1th PC: 2.07
2th PC: 0.53
3th PC: 0.4
```
输出结果告诉我们，第一个主成分就解释了数据中近 70% 的方差。这意味着，即使我们将数据从 3D 降到 1D（只保留第一主成分），也能保留大部分信息。

### 2.1.5 验证：与 Scikit-Learn 对比

为了确保我们从零实现的算法是正确的，可以将其结果与业界标准库 `scikit-learn` 进行比较。

```python
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_centered = scaler.fit_transform(X)

pca_sk = PCA(n_components=dims)
projected_sk = pca_sk.fit_transform(X_centered)
eig_vecs_sk = pca_sk.components_

print("\nScikit-learn hyperplanes:")
for i in range(dims):
    print(f"hyperplane {i}: {eig_vecs_sk[i]}")

# 我们的实现输出的超平面
# hyperplane 0: [-0.58180084 -0.55533668 -0.59422972]
# hyperplane 1: [ 0.51390531 -0.81729222  0.26064299]
# hyperplane 2: [-0.63040394 -0.1537355   0.76089176]

# Scikit-learn 的输出
# hyperplane 0: [ 0.58180084  0.55533668  0.59422972]
# hyperplane 1: [-0.51390531  0.81729222 -0.26064299]
# hyperplane 2: [-0.63040394 -0.1537355   0.76089176]
```
对比发现，特征向量的方向完全一致（符号可能相反，但这不影响方向），证明我们的实现是正确的。

---

## 2.2 K-Means 聚类分析

K-Means 是一种非常流行的无监督聚类算法，它的核心思想是将数据点分配给预设数量（K）的聚类中心（质心）。这是一个快速有效地对数据进行分组、识别群体结构甚至发现异常值的方法。

**优点**:
*   算法简单，易于实现。
*   对于球状的数据簇，效果非常好。

**缺点**:
*   需要预先指定聚类数量 K。
*   对初始质心的选择很敏感。
*   对于非球形的簇或大小不一的簇，效果不佳。

### 2.2.1 任务设定与数据准备

我们首先在一个二维平面上生成一组随机分布的数据点，然后应用 K-Means 算法来发现其中隐藏的聚类。

```python
import numpy as np
import matplotlib.pyplot as plt
import scienceplots
from IPython.display import Image
from celluloid import Camera

np.random.seed(0)
plt.style.use(["science", "no-latex"])

# 定义参数
K = 12  # 我们将测试的最大聚类数
w, h = 1200, 675 # 画布大小
nums = 100 # 数据点数量

# 生成随机数据点
x = np.random.randint(0, w, size=nums)
y = np.random.randint(0, h, size=nums)
pts = np.column_stack((x, y))

# 绘制数据点
fig = plt.figure()
ax = fig.add_subplot(111)
ax.scatter(x, y)
ax.set_title("Randomly Generated Data Points")
plt.show()
```
![Random Data Points](../_images/3e7f1631303df4a92da22979173bcf279543082455661e41f2b5f8474351cd63.png)

### 2.2.2 核心概念：距离的度量

K-Means 算法的核心是“距离”。一个点会被分配给离它最近的那个质心。因此，我们需要一个明确的方式来度量两点之间的距离。

给定两个点 $p_1 = (x_1, y_1)$ 和 $p_2 = (x_2, y_2)$，常用的距离函数有两种：

1.  **欧几里得距离 (Euclidean Distance)**：两点之间的直线距离。
    $$
    d = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}
    $$

2.  **曼哈顿距离 (Manhattan Distance)**：两点在坐标轴上距离的总和，就像在城市街区行走。
    $$
    d = |x_1 - x_2| + |y_1 - y_2|
    $$

在本次实现中，我们使用更常见的欧几里得距离。

```python
def euclidean_distance(p1, p2):
    return np.sqrt((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2)

def manhattan_distance(p1, p2):
    return abs(p1[0] - p2[0]) + abs(p1[1] - p2[1])
```

### 2.2.3 算法流程与代码实现

K-Means 的算法流程是一个迭代的过程：

1.  **初始化**: 随机选择 K 个点作为初始的聚类质心。
2.  **分配 (Assignment Step)**: 对于每一个数据点，计算它到 K 个质心的距离，并将其分配给距离最近的那个质心所在的簇。
3.  **更新 (Update Step)**: 对于每一个簇，重新计算其质心。新的质心是该簇内所有数据点的平均值。
4.  **重复**: 重复第 2 步和第 3 步，直到质心的位置不再有明显变化，或者达到预设的迭代次数。

下面的代码实现了这个流程，并且为了找到最佳的 K 值，它会从 K=1 开始，逐步增加 K 的值，并对每个 K 值运行 K-Means 算法。

```python
def create_plots_kmeans():
    """创建用于K-Means可视化的画布和子图"""
    fig, ax = plt.subplots(1, 3, figsize=(16 / 9.0 * 4, 4 * 1), layout="constrained")
    fig.suptitle("K-Means Clustering Unsupervised")

    ax[0].set_xlabel("K Clusters", fontweight="normal")
    ax[0].set_ylabel("Sum of Euclidean Distance Squared", fontweight="normal")
    ax[0].set_title("Elbow Method")

    ax[1].axis("off")
    ax[2].axis("off")

    ax[2] = fig.add_subplot(1, 2, 2)
    ax[2].set_xlabel("X")
    ax[2].set_ylabel("Y")
    ax[2].set_title("Centroids")

    camera = Camera(fig)
    return ax[0], ax[2], camera

# 初始化质心和颜色
centroids_x = np.random.randint(0, w, size=K)
centroids_y = np.random.randint(0, h, size=K)
centroids = np.column_stack((centroids_x, centroids_y))
colors = np.random.rand(K, 3)

# ---- 主训练循环 ----
ax0, ax1, camera = create_plots_kmeans()
epochs = 8
dists = np.zeros(K) # 存储每个K值的总距离平方和
dists_idx = np.arange(1, K + 1)

# 为可视化决策边界做准备
boundary_div = 25
x_boundary_inc = int(w / boundary_div)
y_boundary_inc = int(h / boundary_div)
x_boundary = np.linspace(0, w, x_boundary_inc + 1)
y_boundary = np.linspace(0, h, y_boundary_inc + 1)
x_boundary, y_boundary = np.meshgrid(x_boundary, y_boundary)
colors_idx_boundary = np.random.randint(0, K, size=x_boundary.shape)
x_boundary_flat = x_boundary.flatten()
y_boundary_flat = y_boundary.flatten()

for k in range(1, K + 1):
    acc_dist_squared = 0
    # 对当前的k值，运行数个epoch
    for e in range(epochs):
        
        # --- 1. 可视化决策边界 (Voronoi Tesselation) ---
        for index in np.ndindex(x_boundary.shape):
            x = x_boundary[index]
            y = y_boundary[index]
            curr_pt = [x, y]
            min_dist = np.inf
            min_group = 0
            for c in range(k):
                dist = euclidean_distance(curr_pt, centroids[c])
                if dist < min_dist:
                    min_dist = dist
                    min_group = c
            colors_idx_boundary[index] = min_group
        colors_boundary = colors[colors_idx_boundary.flatten()]
        ax1.scatter(x_boundary_flat, y_boundary_flat, c=colors_boundary, s=20, alpha=0.45)

        # --- 2. 分配步骤 ---
        groups = [[] for _ in range(k)]
        acc_dist_squared = 0
        for i in range(nums):
            # 找到最近的质心
            min_dist = np.inf
            min_group = 0
            for c in range(k):
                dist = euclidean_distance(pts[i], centroids[c])
                if dist < min_dist:
                    min_dist = dist
                    min_group = c
            groups[min_group].append(pts[i])
            acc_dist_squared += min_dist**2

        # --- 3. 更新步骤与可视化 ---
        for g in range(k):
            # 绘制质心
            ax1.scatter(centroids[g][0], centroids[g][1], color=colors[g], s=8, edgecolors='black')

            group_pts = np.array(groups[g])
            if group_pts.size != 0:
                # 绘制数据点与质心的连线
                for i in range(group_pts.shape[0]):
                    group_pt = group_pts[i]
                    ax1.plot([group_pt[0], centroids[g][0]], [group_pt[1], centroids[g][1]], color=colors[g], linewidth=2, alpha=0.55)
                
                # 更新质心
                centroids[g] = np.mean(group_pts, axis=0)

        # 绘制原始数据点
        ax1.scatter(pts[:, 0], pts[:, 1], c="black", s=15, alpha=0.3)
        # 绘制手肘法图
        if k - 2 > 0:
            ax0.plot(dists_idx[: k - 1], dists[: k - 1], color="red")
        
        # 捕捉动画帧
        camera.snap()

    # 记录该k值下的总距离平方和
    dists[k - 1] = acc_dist_squared / nums
    print(f"K={k}, Avg Sum of Squared Dist: {dists[k-1]:.2f}")

# --- 4. 生成并保存动画 ---
animation = camera.animate()
animation.save("k_means.gif", writer="pillow")
plt.close()
```

### 2.2.4 结果分析：手肘法寻找最佳 K

算法运行后，会生成一个动画，并打印出不同 K 值对应的“簇内平方和 (WCSS)”。

![K-Means Animation](k_means.gif)

```
K=1, Avg Sum of Squared Dist: 158627.63
K=2, Avg Sum of Squared Dist: 62379.95
K=3, Avg Sum of Squared Dist: 47984.72
K=4, Avg Sum of Squared Dist: 40085.60
K=5, Avg Sum of Squared Dist: 28638.72
K=6, Avg Sum of Squared Dist: 24290.07
...
```
动画的左侧图表展示了 **“手肘法” (Elbow Method)**。这个方法将不同 K 值对应的 WCSS 绘制出来。我们可以看到，随着 K 值的增加，WCSS 会不断减小。当 K 值的增加不再导致 WCSS 显著下降时，那个拐点（就像胳膊肘）就是最佳的 K 值。

从我们的输出和动画中的图表可以看出，当 K 从 1 增加到 4 或 5 时，WCSS 下降得非常快。之后，下降速度明显放缓。因此，对于这个随机数据集，选择 4 或 5 作为聚类数量 K 是一个比较合理的选择。这就是手肘法帮助我们确定最佳聚类数量的原理。 