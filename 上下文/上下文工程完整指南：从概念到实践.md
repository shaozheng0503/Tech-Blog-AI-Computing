# 上下文工程完整指南：从概念到实践

## 1. 上下文工程：超越提示工程，推动 AI 发展

### 1.1 如何优化 AI 的"思考"内容

*作者：Dharmesh Shah & LangChain 团队*  
*原文链接：https://simple.ai/p/the-skill-that's-replacing-prompt-engineering & https://blog.langchain.com/the-rise-of-context-engineering/*

如果你已经接触 AI 一段时间了，你就会遇到提示（prompt）的概念——这是你向 ChatGPT 等 LLM 发出指令的方式。

事实证明，你可以采取一些措施来更好地制作提示，以提高结果质量。这种技巧被称为"提示工程"（prompt engineering）——正如它听起来的那样，设计一个提示，让 LLM 做你想做的事。

但有趣的是：在过去两年中，随着上下文窗口从 4K 代币激增到超过 100 万代币，出现了比提示工程更强大的东西：上下文工程（context engineering）。（我想感谢我在 Shopify 的朋友 Tobi 创造了这个术语）

这种转变可能看起来很简单，但框架却完全不同——你现在不是优化你的提问方式，而是优化 AI 在思考时可以访问的内容。

上下文工程是构建动态系统，以正确的格式提供正确的信息和工具，以便 LLM 能够合理地完成任务。大多数情况下，当代理体无法可靠地执行时，根本原因是没有将适当的上下文、指令和工具传达给模型。

LLM 应用程序正在从单个提示演变为更复杂的动态代理系统。因此，上下文工程正在成为 AI 工程师可以培养的最重要的技能。

## 2. 什么是上下文窗口？

### 2.1 上下文窗口的基本概念

把它想象成你交给 LLM 的一张大纸。

你可以在上面写一定数量的字。这些词可以是"write me a limerick about pickleball"（给我写一首关于匹克球的打油诗）。或者，它们可以是"这是一篇 5000 字的文章，给我一个 100 字的总结"（你把整篇文章都写在那张大纸上）。

上下文窗口有一个限制，通常表示为多个"标记"（token）。一个标记大约是一个英语单词的 3/4 或大约 4 个字符。"ChatGPT"是两个代币："Chat"和"GPT"。这很重要，因为计费、延迟和内存都随令牌计数而扩展。

这是 LLM 的一个关键限制：它们只知道它们接受了什么训练，以及上下文窗口中提供给它们的是什么。

但是，随着上下文窗口在过去 2 年中令牌大小的爆炸式增长，我们在该上下文窗口中放置的内容变得越来越聪明，例如：

### 2.2 AI 如何"记住"你的对话

你可能已经注意到，ChatGPT 和 Claude 的短期记忆力很好。这就是为什么你可以提出后续问题，它们知道你在说什么。这是怎么发生的？在幕后，它们实际上是将你之前的提示和输出传递到上下文窗口中。这有点像电影《记忆碎片》，主角忘记了一切，不得不在身上写下笔记，这样他才能记住自己是谁和正在发生的事情（顺便说一句，这是一部很棒的电影）。

### 2.3 RAG：按需教授 AI

有一种称为 RAG（检索增强生成，Retrieval-Augmented Generation）的方法，我们可以找到一组与用户提示相关的文档，然后将这些文档传递到上下文窗口中（从而准确地"教"LLM 有助于回答问题或提示的内容）。

### 2.4 工具调用：扩展 AI 的功能

我们还引入了工具调用（tool calling）的概念。这是一种告诉 LLM 的方式："嘿，假装你可以使用这些工具集进行网络搜索、股票价格查询、天气预报等操作......随便什么。然后，LLM 可以使用提供的一个或多个工具（如果这些工具可以帮助它们处理提示）。

以下是关于工具调用工作原理的一个古怪但令人愉快的细节：当我们让 LLM 知道它有哪些可用的工具时，它实际上并没有调用工具本身。核心 LLM 不是为了做到这一点而构建的。它只是获取上下文并产生输出。我们告诉它可以访问哪些工具，以及输出，它让应用程序（如 ChatGPT）想要调用哪个工具。然后，应用程序代表 LLM 调用该工具，并将结果放在上下文窗口中并将其发送回去。这是解决 LLM 只与上下文窗口一起使用这一事实的聪明方法。

借助百万令牌上下文窗口，你可以向 AI 提供完整的代码库、完整的业务计划或数月的客户支持对话，并且它可以同时对所有这些信息进行推理。

## 3. 什么是上下文工程？

### 3.1 上下文工程的定义与系统特性

上下文工程是构建动态系统，以正确的格式提供正确的信息和工具，以便 LLM 能够合理地完成任务。这个定义建立在 Tobi Lutke、Ankur Goyal 和 Walden Yan 最近对此的看法之上。

复杂代理可能从许多来源获取上下文。上下文可以来自应用程序的开发人员、用户、以前的交互、工具调用或其他外部数据。将所有这些整合在一起涉及一个复杂的系统。其中许多上下文可以动态地出现，因此构建最终提示的逻辑也必须是动态的，它不仅仅是一个静态提示。

### 3.2 上下文工程的核心要素

上下文工程包含三个核心要素：正确的信息、正确的工具和正确的格式。

**正确的信息**：代理系统不执行的一个常见原因是它们没有正确的上下文。LLM 不能读心术——你需要给它们正确的信息。垃圾进，垃圾出。

**正确的工具**：LLM 可能并不总是能够仅根据输入来解决任务。在这些情况下，如果你想授权 LLM 执行此操作，则需要确保它具有正确的工具。这些可以是查找更多信息、采取行动或介于两者之间的任何工具。为 LLM 提供正确的工具与为其提供正确的信息一样重要。

**正确的格式**：就像与人类交流一样，你与 LLM 的交流方式也很重要。简短但描述性的错误消息将比大型 JSON blob 更进一步。这也适用于工具。在确保 LLM 可以使用工具的输入参数时，它们的输入参数非常重要。

### 3.3 故障模式分析与成功标准

在考虑上下文工程时，一个关键问题是：它能合理地完成任务吗？这强调了 LLM 不是读心者——你需要为它们的成功做好准备。它还有助于区分故障模式：它失败是因为你没有给它正确的信息或工具吗？还是它拥有所有正确的信息，但只是搞砸了？这些故障模式的修复方法非常不同。

## 4. 为什么上下文工程很重要

### 4.1 从提示工程到上下文工程的演变

还记得 2023 年的提示工程热潮吗？头条新闻在 Anthropic 等地方为"Prompt Engineers"推销 30 万美元以上的薪水。

很多工作都走红了，因为它们不需要正式的 CS 学位——只需要能说流利的"LLM"的能力。

虽然聘请了一些提示工程师，但最终并没有成为许多人预测的庞大工作类别。原因是什么？每个人都成为了一名提示工程师。现在，出现了更重要的东西：上下文工程。

上下文工程本质上是提示工程的更高级别版本。

一个（过于）简单的解释：提示工程就像学习提出非常好的问题。上下文工程就像一名图书管理员，在人们开始阅读之前就决定他们可以访问哪些书籍。

### 4.2 代理系统失败的根本原因

当代理系统搞砸时，主要是因为 LLM 搞砸了。从第一原则出发，LLM 可能会出错，原因有两个：

1. 底层模型只是搞砸了，它不够好
2. 底层模型没有传递适当的上下文来产生良好的输出

通常情况下（尤其是当模型变得更好时）模型错误更多地是由第二个原因引起的。传递给模型的上下文可能由于以下几个原因而损坏：

- 只是缺少模型做出正确决策所需的上下文。模型不是读心者。如果你不给它们正确的上下文，它们就不会知道它的存在。
- 上下文格式不佳。就像人类一样，沟通也很重要！传入模型时如何格式化数据绝对会影响它的响应方式。

### 4.3 上下文工程师实际做什么

**策展**：确定哪些文档、记忆或 API 对每个特定任务很重要

**结构**：将系统消息→工具分层，→检索到的数据→以最佳顺序用户提示符

**压缩**：汇总或分块信息以保持在令牌限制范围内，同时保留重要内容

**评估**：测量准确性并注意"上下文稀释"，即不相关的信息会分散模型的注意力

请记住，更多的上下文意味着更丰富的文档和更长的对话。但成本和延迟大致随窗口长度线性增加。这为上下文工程师留下了大量空间来发现最佳实践（其中许多仍在兴起）。

上下文工程需要以提示工程从未有过的方式考虑信息架构、数据策略和用户体验。掌握这一点的公司将拥有巨大的竞争优势。

正如我想说的：提示告诉模型如何思考，但上下文工程为模型提供完成工作的训练和工具。

## 5. 上下文工程与提示工程有何不同？

### 5.1 从"prompts"转向"context"

为什么从"prompts"转向"context"？早期，开发人员专注于巧妙地措辞提示，以哄骗更好的答案。但随着应用程序变得越来越复杂，越来越明显的是，为 AI 提供完整和结构化的上下文比任何神奇的措辞都重要得多。

我还想说，提示工程是上下文工程的一个子集。即使你拥有所有的上下文，如何在 Prompt 中组合它仍然绝对重要。区别在于，你不是为了很好地处理一组输入数据来构建 prompt，而是采用一组动态数据并正确格式化它。

我还想强调的是，上下文的一个关键部分通常是 LLM 应该如何表现的核心指令。这通常是提示工程的关键部分。你认为为代理应该如何行为提供清晰详细的说明是上下文工程还是提示工程？我认为两者兼而有之。

## 6. 上下文工程示例

良好的上下文工程的一些基本示例包括：

**工具使用**：确保如果代理需要访问外部信息，它有可以访问这些信息的工具。当工具返回信息时，它们会以 LLM 最容易理解的方式进行格式设置。

**短期记忆**：如果对话正在进行一段时间，请创建对话摘要并在将来使用它。

**长期记忆**：如果用户在之前的对话中表达了偏好，则能够获取该信息。

**提示工程**：提示中清楚地列举了代理应如何行为的说明。

**检索**：动态获取信息，并在调用 LLM 之前将其插入到提示符中。

## 7. 实践工具：LangGraph 和 LangSmith

### 7.1 LangGraph 如何实现上下文工程

当我们构建 LangGraph 时，我们构建它的目标是使其成为最可控的代理框架。这也使其能够完美地实现上下文工程。

使用 LangGraph，你可以控制一切。你可以决定运行哪些步骤。你决定你的 LLM 到底要做什么。你可以决定存储输出的位置。你控制一切。

这使你可以执行所需的所有上下文工程。代理抽象的缺点之一（大多数其他代理框架都强调）是它们限制了上下文工程。在某些地方，你无法准确更改进入 LLM 的内容，或者无法更改预先运行的步骤。

旁注：Dex Horthy 的《12 Factor Agents》是一本非常好的读物。其中的很多要点都与上下文工程有关（"拥有你的提示"、"拥有你的上下文构建"等）。我们真的很喜欢他传达空间中重要事物的方式。

### 7.2 LangSmith 如何帮助进行上下文工程

LangSmith 是我们的 LLM 应用程序可观察性和评估解决方案。LangSmith 的主要功能之一是能够跟踪你的代理调用。尽管在我们构建 LangSmith 时不存在"上下文工程"一词，但它恰如其分地描述了这种跟踪的帮助。

LangSmith 让你看到代理中发生的所有步骤。这样，你就可以查看运行了哪些步骤来收集发送到 LLM 的数据。

LangSmith 允许你查看 LLM 的确切输入和输出。这样，你就可以准确地看到 LLM 中的内容——它拥有的数据以及它是如何格式化的。然后，你可以调试它是否包含任务所需的所有相关信息。这包括 LLM 可以访问哪些工具——因此你可以调试是否为其提供了适当的工具来帮助完成手头的任务。

## 8. 上下文优先的未来

### 8.1 从优化句子到优化知识

2023 年的提示工程炒作教会了我们一个重要的教训：最有价值的 AI 技能不是学习秘密短语或聪明的技巧。

它们是关于了解如何构建能够在正确时间访问正确信息的智能系统。

这是从优化句子到优化知识的根本转变。这对于当今使用 AI 进行构建的任何人都很重要。

就我个人而言，作为与 agent.ai 一起在这个领域构建的人，我总是对当你不再将 AI 视为聊天机器人，开始将其视为具有正确上下文和工具的推理引擎时所发生的事情感到惊讶。

### 8.2 沟通就是你所需要的一切

几个月前，我写了一篇名为"沟通就是你所需要的一切"的博客。要点是与 LLM 沟通很困难，而且没有得到足够的重视，而且通常是许多代理错误的根本原因。其中许多要点都与上下文工程有关！

上下文工程并不是一个新想法——代理构建者在过去一两年里一直在这样做。这是一个新术语，恰如其分地描述了一项日益重要的技能。我们将撰写和分享有关此主题的更多信息。我们认为我们构建的许多工具（LangGraph、LangSmith）都是为实现上下文工程而构建的，因此我们很高兴看到人们对此的重视。

提示工程时代教会我们与 AI 对话。上下文工程时代正在教会我们用 AI 思考。

---

*原文作者：Dharmesh Shah（HubSpot 联合创始人兼首席技术官）& LangChain 团队*  
*翻译与本地化：面向中文 AI 开发者社区* 